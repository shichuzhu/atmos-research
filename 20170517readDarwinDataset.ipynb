{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T03:41:16.160252Z",
     "start_time": "2017-06-16T03:40:49.815Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob, re, os\n",
    "import scipy\n",
    "from scipy import io, optimize\n",
    "import pickle\n",
    "import datetime\n",
    "import netCDF4\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings('default') # restore default settings\n",
    "\n",
    "## For debug mode\n",
    "from IPython.core.debugger import Tracer\n",
    "#Tracer()() #this one triggers the debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-04T06:29:35.596224Z",
     "start_time": "2017-06-04T06:29:35.586299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128.704\n"
     ]
    }
   ],
   "source": [
    "# Check memory usage\n",
    "import resource\n",
    "print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-05T21:34:00.536837Z",
     "start_time": "2017-06-05T21:34:00.505709Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# meta data for defining time range of flights\n",
    "metatmp = [\n",
    "    #                           27,    35,     42,44\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140116_fs140001.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140116_fs140002.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140117_fs140003.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140118_fs140004.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140121_fs140005.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140123_fs140006.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140124_fs140007.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140127_fs140008.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140128_fs140009.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140129_fs140010.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140130_fs140011.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140202_fs140012.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140203_fs140013.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140204_fs140014.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140205_fs140015.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140207_fs140016.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140208_fs140017.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140208_fs140018.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140209_fs140019.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140210_fs140020.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140217_fs140021.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140217_fs140022.txt',\n",
    "    'F20_1Hz-HAIC_base_aipov_v4_20140218_fs140023.txt']\n",
    "\n",
    "# meta = [(tmp[26:34], int(tmp[41:43])) for tmp in metatmp]\n",
    "meta = {int(tmp[42:44]):tmp[27:35] for tmp in metatmp}\n",
    "\n",
    "outputfile = 'tmp/darwin_bulk.nc'\n",
    "# cayenne_bulk.nc  required for generating time-sync files\n",
    "# cayenne_sync.nc  huge time sync file containing all data\n",
    "# cayenne_sync_bulk.nc  all data except lamp psd\n",
    "# cayenne_sync_gz.nc  zlib compressed lamp psd only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T14:31:45.604582Z",
     "start_time": "2017-05-18T14:31:39.099662Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning!! Duplicate data in saffire is found, dropping ->\n",
      "1\n",
      "Warning!! Duplicate data in saffire is found, dropping ->\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## For SAFFIRE V4 data\n",
    "def read_saffire(filepath):\n",
    "    tmp = os.path.basename(filepath)\n",
    "    tmpflight = int(tmp[42:44])\n",
    "    tmpdate = meta[tmpflight]\n",
    "    '''\n",
    "    Things to note\n",
    "    1. The time is in seconds and can be non-sharp seconds.\n",
    "    '''\n",
    "    varnames = [\n",
    "        'Timeinsecond',\n",
    "        'event_marker',\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'altitude_gps',\n",
    "        'altitude_airins',\n",
    "        'platform_roll_angle',\n",
    "        'platform_pitch_angle',\n",
    "        'platform_orientation',\n",
    "        'air_pressure',\n",
    "        'air_temperature_rm',\n",
    "        'air_temperature_impact',\n",
    "        'air_temperature_adc',\n",
    "        'dew_point_temperature',\n",
    "        'relative_humidity',\n",
    "        'humidity_mixing_ratio_hygrometer',\n",
    "        'humidity_mixing_ratio_aerodata',\n",
    "        'humidity_mixing_ratio_wvss2',\n",
    "        'platform_speed_wrt_air',\n",
    "        'platform_acceleration',\n",
    "        'platform_course',\n",
    "        'platform_speed_wrt_ground_aipov',\n",
    "        'platform_course',\n",
    "        'platform_speed_wrt_ground_gps',\n",
    "        'upward_platform_speed_wrt_ground',\n",
    "        'angle_of_attack',\n",
    "        'angle_of_sideslip',\n",
    "        'eastward_wind',\n",
    "        'northward_wind',\n",
    "        'upward_air_velocity',\n",
    "        'wind_from_direction',\n",
    "        'wind_speed',\n",
    "        'mic_msofreqice_rs_sync_1'\n",
    "    ]\n",
    "\n",
    "    lookup = 'Warning : most measurements are not valid before take-off and after landing'\n",
    "    comments = []\n",
    "    with open(filepath) as myFile:\n",
    "        for num, line in enumerate(myFile, 1):\n",
    "            if lookup in line:\n",
    "                skipline = num\n",
    "                break\n",
    "    ### r\"\\s+\" refers to one or more occurences of whitespace, while r\"\\s*\" will match zero and would raise a warning\n",
    "    tmp = pd.read_csv(filepath,skiprows=skipline,names=varnames,sep=r\"\\s+\",na_values=3.40282347e+38)\n",
    "\n",
    "    # pd.TimedeltaIndex(round(tmp.Timeinsecond).astype(int),units='s')\n",
    "    tmp.loc[:,'time'] = (pd.TimedeltaIndex(tmp.Timeinsecond,unit='s') + pd.Timestamp(tmpdate)).round('s')\n",
    "    tmp.set_index(keys='time',inplace=True,verify_integrity=False)\n",
    "    tmp = tmp[pd.notnull(tmp.index)]\n",
    "    tmp.index.rename('timeutc',inplace=True)\n",
    "\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in saffire is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    tmp['flightnum'] = tmpflight\n",
    "    return tmp\n",
    "filelist = glob.glob('/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-*')\n",
    "filelist.sort()\n",
    "saffireset = [ read_saffire(filepath) for filepath in filelist ]\n",
    "# astype does not provide a 'coerce' option\n",
    "datasetsaffire = pd.DataFrame().append(saffireset)\n",
    "datasetsaffire.to_xarray().to_netcdf(outputfile,mode='w',\n",
    "    format='NETCDF4',engine='netcdf4',group='/saffire',unlimited_dims=['timeutc'])\n",
    "# datasetsaffire.apply(pd.to_numeric,errors='raise').dropna(how='all').equals(datasetsaffire) -> gives a True\n",
    "\n",
    "# Create metaspan useful for files not named by flight number\n",
    "test = datasetsaffire.flightnum\n",
    "def tmpfun(flt):\n",
    "    tmp = test[test == flt].index\n",
    "    return [flt,min(tmp),max(tmp)]\n",
    "metaspan = list(map(tmpfun,range(1,24)))\n",
    "takeofftime = pd.DatetimeIndex([x[1] for x in metaspan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T14:31:52.981418Z",
     "start_time": "2017-05-18T14:31:51.253793Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning!! Duplicate data in IKP is found, dropping ->\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Read IKP\n",
    "# with open('tmp/ikpdata.p','rb') as fin:\n",
    "#     datasetipk = pickle.load(fin)\n",
    "\n",
    "def read_ikp2(filepath):\n",
    "    tmp = os.path.basename(filepath)\n",
    "    ts = tmp\n",
    "    tsflt = pd.Timestamp(ts[4:8]+ts[9:11]+ts[12:14]+ts[15:21])\n",
    "    tmpflight = argmin(abs(tsflt - takeofftime))+1\n",
    "        \n",
    "    tmpdate = meta[tmpflight]\n",
    "    # Skip the first row of each csv file. Warning this would lose data! Otherwise data are read as string, not float\n",
    "    # tmp = pd.read_csv(filepath, header=4, skiprows=1, na_values=-999.0)\n",
    "    \n",
    "    # Weird line 7 ending comma bug leads to discarding first two lines of data.\n",
    "    delrow = list(range(7))\n",
    "    delrow.remove(4)\n",
    "    tmp = pd.read_csv(filepath, header=0, skiprows=delrow, na_values=-999.0)\n",
    "    \n",
    "    tmp.loc[:,'Stimech'] = pd.Timestamp(tmpdate) + pd.to_timedelta(tmp['Stimech'],errors='coerce') # Get rid of null lines\n",
    "    tmp.set_index(keys='Stimech',inplace=True,verify_integrity=False)\n",
    "    tmp = tmp[pd.notnull(tmp.index)]\n",
    "    tmp.index.rename('timeutc',inplace=True)\n",
    "\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in IKP is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    return tmp\n",
    "filelist = glob.glob('/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/ikp/f20-2014*')\n",
    "filelist.sort()\n",
    "ikpset = [ read_ikp2(filepath) for filepath in filelist ]\n",
    "\n",
    "datasetipk = pd.DataFrame().append(ikpset).astype(float).replace(-999., nan)\n",
    "datasetipk.to_xarray().to_netcdf(outputfile,mode='a', ## 'w' for first time, later change to 'a'\n",
    "    format='NETCDF4',engine='netcdf4',group='/ikp',unlimited_dims=['timeutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T14:32:16.300430Z",
     "start_time": "2017-05-18T14:32:07.178351Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning!! Duplicate data in Robust is found, dropping ->\n",
      "2\n",
      "Warning!! Duplicate data in Robust is found, dropping ->\n",
      "1\n",
      "Warning!! Duplicate data in Robust is found, dropping ->\n",
      "1\n",
      "Warning!! Duplicate data in Robust is found, dropping ->\n",
      "2\n",
      "Warning!! Duplicate data in Robust is found, dropping ->\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Read robust\n",
    "def read_robust(filepath):\n",
    "    global meta\n",
    "    tmp = os.path.basename(filepath)\n",
    "\n",
    "    tmpflight = int(tmp[15:17])\n",
    "    tmpdate = meta[tmpflight]\n",
    "    # Skip the first row of each csv file. Warning this would lose data! Otherwise data are read as string, not float\n",
    "    # tmp = pd.read_csv(filepath, header=4, skiprows=1, na_values=-999.0)\n",
    "    tmp = pd.read_excel(filepath)\n",
    "    tmp.columns = ['time','TWC_robust']\n",
    "    # excel files have date and time ready\n",
    "    tmp.loc[:,'time'] = pd.DatetimeIndex(tmp['time'],errors='coerce').round('1S') # Get rid of null lines\n",
    "    tmp.set_index(keys='time',inplace=True,verify_integrity=False)\n",
    "    tmp = tmp[pd.notnull(tmp.index)]\n",
    "    tmp.index.rename('timeutc',inplace=True)\n",
    "\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in Robust is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    return tmp\n",
    "\n",
    "filelist = glob.glob('/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/dataHAIC/Robust_data_flt*')\n",
    "filelist.sort()\n",
    "robustset = [ read_robust(filepath) for filepath in filelist ]\n",
    "# astype does not provide a 'coerce' option\n",
    "datasetrobust = pd.DataFrame().append(robustset).apply(pd.to_numeric, axis=0, errors='coerce').dropna()\n",
    "datasetrobust.to_xarray().to_netcdf(outputfile,mode='a',\n",
    "    format='NETCDF4',engine='netcdf4',group='/robust',unlimited_dims=['timeutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T14:33:29.167591Z",
     "start_time": "2017-05-18T14:33:21.210466Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning!! Duplicate data in saffire is found, dropping ->\n",
      "1\n",
      "Warning!! Duplicate data in saffire is found, dropping ->\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## CDP Conc\n",
    "def read_cdpconc(filepath,tmpflight):\n",
    "    tmp = os.path.basename(filepath)\n",
    "    tmpdate = meta[tmpflight]\n",
    "    tmp = pd.read_excel(filepath)\n",
    "    tmp.columns = ['time','cdpconcpercm3']\n",
    "\n",
    "    tmp.loc[:,'time'] = (pd.TimedeltaIndex(tmp.time,unit='s') + pd.Timestamp(tmpdate)).round('s')\n",
    "    tmp.set_index(keys='time',inplace=True,verify_integrity=False)\n",
    "    tmp = tmp[pd.notnull(tmp.index)]\n",
    "    tmp.index.rename('timeutc',inplace=True)\n",
    "\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in saffire is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    return tmp\n",
    "\n",
    "# Need regular expression for flight number as they are not fixed width\n",
    "tmpfileset = [ ( int(re.search('(?<=CDP3V)\\d*(?=\\.xls)', os.path.basename(filepath)).group()), filepath ) for filepath \n",
    "          in glob.glob('/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/dataHAIC/HAIC2014-Data-CDP3*') ]\n",
    "tmpfileset.sort()\n",
    "tmpset = [ read_cdpconc(fileinfo[1],fileinfo[0]) for fileinfo in tmpfileset ]\n",
    "# dropna applies changes\n",
    "datasetcdpconc = pd.DataFrame().append(tmpset).apply(pd.to_numeric,errors='raise').dropna(how='all')\n",
    "datasetcdpconc.to_xarray().to_netcdf(outputfile,mode='a',\n",
    "    format='NETCDF4',engine='netcdf4',group='/cdp',unlimited_dims=['timeutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T14:33:41.699545Z",
     "start_time": "2017-05-18T14:33:41.610433Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate sorted fileinfo for lamp PSD & MSD. keyword: re regular expression\n",
    "def tmpfun(filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    tmpflight = int(re.search('(?<=vol)\\d*(?=\\.txt)',filename).group())\n",
    "    # The (?:.(?!-))* is for non-greedy matching, reference http://stackoverflow.com/a/2527791/5426033\n",
    "    signit = re.search('(?<=-)(?:.(?!-))*(?=_vol)',filename).group()\n",
    "    key = dict(zip(['Deq','Dmax','ly'],['psddeq','psddmax','psdly']))[signit]\n",
    "    return (key,tmpflight,filepath)\n",
    "finfo1 = list(map(tmpfun,glob.glob('/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/lamp/Composite_2DS-PIP*')))\n",
    "\n",
    "def tmpfun(filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    tmpflight = int(re.search('(?<=F#)\\d*(?=-Mass)',filename).group())\n",
    "    signit = re.search('(?<=SizeD_).*(?=_2DS)',filename).group()\n",
    "    key = dict(zip(['Deq','Dmax'],['msddeq','msddmax']))[signit]\n",
    "    return (key,tmpflight,filepath)\n",
    "finfo2 = list(map(tmpfun,glob.glob(\n",
    "    '/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20151203/dataHAIC/*MassSize*')))\n",
    "finfo = finfo1 + finfo2\n",
    "finfo.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T14:35:01.928590Z",
     "start_time": "2017-05-18T14:33:45.850248Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## LAMP PSD\n",
    "def read_lamppsd(filepath,tmpflight,key):\n",
    "    tmpdate = meta[tmpflight]\n",
    "    tmp = pd.read_csv(filepath,sep=r\"\\s+\",header=None,skiprows=1)\n",
    "\n",
    "    tmp.iloc[:,0] = pd.TimedeltaIndex(tmp.iloc[:,0],unit='s') + pd.Timestamp(tmpdate)\n",
    "    tmp.set_index(keys=0,inplace=True,verify_integrity=False)\n",
    "    tmp = tmp[pd.notnull(tmp.index)]\n",
    "    tmp.index.rename('timeutc',inplace=True)\n",
    "\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in lamppsd is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    tmp = pd.concat([tmp], axis=1, keys=[key]) # Add the outmost row index flight number\n",
    "    return tmp\n",
    "\n",
    "bunch = [ read_lamppsd(x[2],x[1],x[0]) for x in finfo ]\n",
    "\n",
    "lamppd = pd.concat( [ pd.DataFrame().append([bunch[finfo.index(x)] for x in finfo if x[0] == y ]) \n",
    "       for y in set([x[0] for x in finfo]) ] , axis=1 )\n",
    "\n",
    "# with open('tmp/lamptmp.p','wb') as fout:\n",
    "#     pickle.dump(lamppd,fout)\n",
    "\n",
    "# with open('tmp/lamptmp.p','rb') as fin:\n",
    "#     lamppd = pickle.load(fin)\n",
    "\n",
    "# Since ly has different bin_div against other PSD, it is taken out to deal with alone.\n",
    "partlist = list(lamppd.columns.levels[0])\n",
    "partlist.remove('psdly')\n",
    "\n",
    "b = pd.Panel({x:lamppd[x] for x in partlist}).to_xarray()\n",
    "b = b.rename({'major_axis':'timeutc','minor_axis':'bin_mid_composite'}).to_dataset(dim='items')\n",
    "\n",
    "b['bin_mid_composite'] = (1+np.arange(len(b.bin_mid_composite)))*10+5.\n",
    "b.attrs['bin_div_composite'] = (1+np.arange(1+len(b.bin_mid_composite)))*10.\n",
    "bt = b\n",
    "\n",
    "b = pd.Panel({'psdly':lamppd['psdly']}).to_xarray()\n",
    "b = b.rename({'major_axis':'timeutc','minor_axis':'bin_mid_ly'}).to_dataset(dim='items')\n",
    "\n",
    "fn = '/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/lamp/Composite_2DS-PIP-Intercomp_PSD-ly_vol01.txt'\n",
    "titletmp = pd.read_table(fn)\n",
    "b['bin_mid_ly'] = np.array(titletmp.columns.values)[1:].astype(float)\n",
    "bin_mid = np.array(b['bin_mid_ly'])\n",
    "bin_div = np.append(np.empty_like(bin_mid),nan)\n",
    "bin_div[0] = 10\n",
    "for i in range(len(bin_mid)):\n",
    "    bin_div[i+1] = 2*bin_mid[i]-bin_div[i]\n",
    "b.attrs['bin_div_ly'] = bin_div\n",
    "\n",
    "datasetlamp = xr.merge([b,bt])\n",
    "datasetlamp.to_netcdf(outputfile,mode='a',\n",
    "    format='NETCDF4',engine='netcdf4',group='/lamp',unlimited_dims=['timeutc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T14:45:01.868368Z",
     "start_time": "2017-05-18T14:42:35.467161Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge all data to one time dimension\n",
    "outputfile = 'tmp/darwin_sync_bulk.nc'\n",
    "outputfile1 = 'tmp/darwin_sync_gz.nc'\n",
    "setnamelist = ['saffire','ikp','robust','cdp','lamp']\n",
    "# setnamelist = ['saffire','ikp','robust','cdp']\n",
    "alldata = []\n",
    "for name in setnamelist:\n",
    "    alldata.append( xr.open_dataset('tmp/darwin_bulk.nc',group=name) )\n",
    "\n",
    "alltimelist = [x.timeutc for x in alldata]\n",
    "alltime = unique(xr.concat(alltimelist,dim='timeutc'))\n",
    "alltime = xr.DataArray(alltime,dims={'timeutc':len(alltime)},coords={'timeutc':alltime})\n",
    "alltime = alltime.to_dataset(name='time_to_drop')\n",
    "\n",
    "for i in range(len(setnamelist)-1):\n",
    "    name = setnamelist[i]\n",
    "#     mode = 'a'\n",
    "    if i == 0:\n",
    "        mode = 'w'\n",
    "    else:\n",
    "        mode = 'a'\n",
    "    mergebulk = xr.merge([alltime,alldata[i]]).drop('time_to_drop')\n",
    "    mergebulk.to_netcdf(outputfile,mode=mode,\n",
    "    format='NETCDF4',engine='netcdf4',group='/'+name,unlimited_dims=['timeutc'])\n",
    "\n",
    "i = 4\n",
    "name = setnamelist[i]\n",
    "mergepsd = xr.merge([alltime,alldata[i]]).drop('time_to_drop')\n",
    "mergepsd.to_netcdf(outputfile1,mode='w',\n",
    "    format='NETCDF4',engine='netcdf4',group='/'+name,unlimited_dims=['timeutc'],\n",
    "    encoding = {x:{'zlib':True} for x in ['msddeq','msddmax','psddeq','psddmax','psdly']} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T14:55:43.986933Z",
     "start_time": "2017-05-18T14:55:43.929427Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Elaborate coordinates and attributes\n",
    "totalbins = 1284\n",
    "bin_div = 10*np.arange(totalbins+1)+10\n",
    "bin_mid = (bin_div[:-1]+bin_div[1:])/2\n",
    "\n",
    "# outputfile1 = 'tmp/darwin_sync_gz.nc' created in the previous cell\n",
    "with netCDF4.Dataset(outputfile1,'a') as dset:\n",
    "    dset['/lamp/bin_mid_composite'][:] = bin_mid\n",
    "    dset['/lamp'].bin_div = bin_div.astype(float)\n",
    "    dset['/lamp'].bin_div_units = 'um'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T18:05:20.866689Z",
     "start_time": "2017-05-18T18:05:20.712936Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### IGF routine\n",
    "f_mygamma = lambda nml, x: 10**nml[0]*x**nml[1]*np.exp(-nml[2]*x)\n",
    "def f_one_mode(psd, bin_div, moments):\n",
    "    # bin_diff = np.diff(bin_div)\n",
    "    # bin_mid = (bin_div[:-1]+bin_div[1:])/2.\n",
    "    mobs = np.empty(moments.shape)\n",
    "    for szmoment in range(len(mobs)):\n",
    "        mobs[szmoment] = np.nansum( psd*bin_diff*bin_mid**(moments[szmoment]) )\n",
    "\n",
    "    x0 = np.array([log10(300), -1, 0.0014])\n",
    "    # any zero bins as well as nan will be ignored\n",
    "    tmpind = psd>0\n",
    "    tmpfun = lambda x, nml1, nml2, nml3: f_mygamma(np.array([nml1,nml2,nml3]),x)\n",
    "    nml0, pcov = scipy.optimize.curve_fit(tmpfun, bin_mid[tmpind], psd[tmpind], p0=x0)\n",
    "    # nml0 = [1.8744, -0.6307, 0.0033]\n",
    "    f_fit = lambda nml: f_sum_chisquare(nml, moments, bin_mid, bin_diff, mobs)\n",
    "    optresult = scipy.optimize.minimize(f_fit, nml0, method='Nelder-Mead')\n",
    "    return optresult\n",
    "    \n",
    "def f_sum_chisquare( nml, moments, bin_mid, bin_diff, mobs ):\n",
    "    psd = f_mygamma(nml, bin_mid)\n",
    "    ### We may drop this condition if the code works fine.\n",
    "#     if any(np.isnan(psd)) or any(np.isinf(psd)):\n",
    "#         return np.inf\n",
    "    mfit = np.empty(mobs.shape)\n",
    "    for szmoment in range(len(mfit)):\n",
    "        mfit[szmoment] = np.sum( psd*bin_diff*bin_mid**(moments[szmoment]) )\n",
    "    return np.sum( (mfit-mobs)**2/mobs/mfit )\n",
    "\n",
    "## For median mass diameter calculation\n",
    "def f_mmd(bin_div, msd):\n",
    "    # bin_diff = np.diff(bin_div)\n",
    "    cmsd = np.concatenate( (np.array([0]),np.cumsum(msd*bin_diff)) )\n",
    "    if cmsd[-1]<=0:\n",
    "        return np.NaN\n",
    "    cmsd /= cmsd[-1]\n",
    "    indtmp = np.where(np.diff(cmsd>0.5)==1)[0]\n",
    "#     if len(indtmp)>1:\n",
    "#         print(indtmp)\n",
    "    x1,x2,y1,y2 = bin_div[indtmp], bin_div[indtmp+1], cmsd[indtmp], cmsd[indtmp+1]\n",
    "    mmd = (x2-x1)/(y2-y1)*(0.5-y1)+x1\n",
    "    return mmd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T17:50:08.445675Z",
     "start_time": "2017-05-18T17:49:34.814300Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "# IGF fit\n",
    "# outputfile = 'tmp/darwin_sync_bulk.nc'\n",
    "# outputfile1 = 'tmp/darwin_sync_gz.nc'\n",
    "a = xr.open_dataset(outputfile1,group='/lamp')\n",
    "bin_div = a.bin_div\n",
    "bin_diff = np.diff(bin_div)\n",
    "bin_mid = (bin_div[:-1]+bin_div[1:])/2.\n",
    "\n",
    "## IGF fit and save the data\n",
    "moments = np.array([0,2,3])\n",
    "PSDs = a['psddmax'].values\n",
    "MSDs = a['msddmax'].values\n",
    "shp = PSDs.shape\n",
    "# At least 11 non-zero bins are required for a valid PSD\n",
    "# Since last few bins of PSD is alwasys nan, use 'not all are nan' as nalid PSD index\n",
    "validpsdbool = (~all(isnan(PSDs),axis=1)) & (sum(PSDs>0,axis=1)>10)\n",
    "validpsdind = np.where(validpsdbool)[0]\n",
    "\n",
    "'''\n",
    "import signal\n",
    "class TimeoutException(Exception):   # Custom exception class\n",
    "    pass\n",
    "def timeout_handler(signum, frame):   # Custom signal handler\n",
    "    raise TimeoutException\n",
    "# Change the behavior of SIGALRM\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "for i in range(3):\n",
    "    # Start the timer. Once 5 seconds are over, a SIGALRM signal is sent.\n",
    "    signal.alarm(5)    \n",
    "    # This try/except loop ensures that \n",
    "    #   you'll catch TimeoutException when it's sent.\n",
    "    try:\n",
    "        A(i) # Whatever your function that might hang\n",
    "    except TimeoutException:\n",
    "        continue # continue the for loop if function A takes more than 5 second\n",
    "    else:\n",
    "        # Reset the alarm\n",
    "        signal.alarm(0)\n",
    "'''\n",
    "# IGF fit and save data\n",
    "\n",
    "output = {}\n",
    "i = 0\n",
    "totali = len(validpsdind)\n",
    "for szi in validpsdind:\n",
    "    i+=1\n",
    "    psd = PSDs[szi,:]\n",
    "    print(str(szi)+' ... ', i, totali, '{:.2f}'.format(i/totali*100),end=\"\")\n",
    "    try:\n",
    "        output[szi]=f_one_mode(psd,bin_div,moments)\n",
    "        print('\\r'+str(szi)+' Done. Now',end=\"\")\n",
    "    except:\n",
    "        output[szi]=None\n",
    "        print('\\r'+str(szi)+' Error. Now',end=\"\")\n",
    "print('Finished!')\n",
    "with open('tmp/output.p', 'wb') as file:\n",
    "    pickle.dump(output,file)\n",
    "    print('Output file saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T18:02:21.298990Z",
     "start_time": "2017-05-18T18:02:17.171576Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the output fit file and save to cayenne_sync_bulk.nc as /lampproc group\n",
    "outputfile = 'tmp/darwin_sync_bulk.nc'\n",
    "outputfile1 = 'tmp/darwin_sync_gz.nc'\n",
    "with open('tmp/output.p', 'rb') as file:\n",
    "    output = pickle.load(file)\n",
    "    print('Output file read')\n",
    "\n",
    "with netCDF4.Dataset(outputfile1, \"r\", format=\"NETCDF4\") as file:\n",
    "    varname = list(file.groups['lamp'].variables.keys())\n",
    "    varname.remove('timeutc')\n",
    "frame = xr.open_dataset(outputfile1,group='/lamp',drop_variables=varname)\n",
    "\n",
    "tmpnml = np.empty((frame.dims['timeutc'],3), dtype=float)\n",
    "tmpnml[:] = nan\n",
    "# Remove entries that are None\n",
    "output = {k:v.x for k,v in output.items() if v is not None}\n",
    "tmpnml[ list(output.keys()),:] = np.array([x for x in output.values()])\n",
    "tmpvalidbinnum = np.empty(PSDs.shape[0])\n",
    "tmpvalidbinnum[:] = nan\n",
    "mask = ~all(isnan(PSDs),axis=1)\n",
    "tmpvalidbinnum[mask] = np.sum(PSDs[mask,:]>0,axis=1)\n",
    "frame['validbinnum'] = xr.DataArray(tmpvalidbinnum,dims=['timeutc'])\n",
    "frame['nml'] = xr.DataArray(tmpnml,dims=['timeutc','dimnml'])\n",
    "\n",
    "frame.to_netcdf(outputfile,mode='a',\n",
    "    format='NETCDF4',engine='netcdf4',group='/lampproc',unlimited_dims=['timeutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add mmd to cayenne_sync_bulk.nc, also as an template for adding variable\n",
    "\n",
    "# find valid msd index here. MSD is not exactly same as PSD in Darwin dataset\n",
    "validmsdbool = (~all(isnan(MSDs),axis=1)) & (sum(MSDs>0,axis=1)>10)\n",
    "validmsdind = np.where(validmsdbool)[0]\n",
    "\n",
    "tmpmmd = np.empty(PSDs.shape[0])\n",
    "tmpmmd[:] = nan\n",
    "validMSDs = MSDs[validmsdind,:]\n",
    "validmmd = np.array([f_mmd(bin_div,x) for x in validMSDs])\n",
    "tmpmmd[validmsdind] = validmmd\n",
    "\n",
    "# As a template to add new variable to existing netcdf file instead of using xarray\n",
    "file = netCDF4.Dataset(outputfile,mode='a')\n",
    "grp = file['/lampproc']\n",
    "# The extra comma makes sure the passed constant is a tuple as required\n",
    "# Once variable created, the file will reject repeated creation\n",
    "try:\n",
    "    varmmd = grp.createVariable(\"mmd\",\"f8\",('timeutc',))\n",
    "except RuntimeError:\n",
    "    varmmd = grp['mmd']\n",
    "varmmd[:] = tmpmmd\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T06:34:30.432220Z",
     "start_time": "2017-06-02T06:15:08.328289Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing key latitude ... 1/94\n",
      "Processing key longitude ... 2/94\n",
      "Processing key altitude ... 3/94\n",
      "Processing key pitch ... 4/94\n",
      "Processing key roll ... 5/94\n",
      "Processing key drift ... 6/94\n",
      "Processing key heading ... 7/94\n",
      "Processing key track ... 8/94\n",
      "Processing key aircraft_vh ... 9/94\n",
      "Processing key aircraft_vz ... 10/94\n",
      "Processing key pressure ... 11/94\n",
      "Processing key temperature ... 12/94\n",
      "Processing key relative_humidity ... 13/94\n",
      "Processing key eastward_wind ... 14/94\n",
      "Processing key northward_wind ... 15/94\n",
      "Processing key u_wind ... 16/94\n",
      "Processing key v_wind ... 17/94\n",
      "Processing key w_wind ... 18/94\n",
      "Processing key u_wind_fuselage ... 19/94\n",
      "Processing key v_wind_fuselage ... 20/94\n",
      "Processing key land_water_flag ... 21/94\n",
      "Processing key height_2D ... 22/94\n",
      "Processing key Z_vertical ... 23/94\n",
      "Processing key Z_L1_vertical ... 24/94\n",
      "Processing key V_vertical ... 25/94\n",
      "Processing key R_vertical ... 26/94\n",
      "Processing key latitude_vertical ... 27/94\n",
      "Processing key longitude_vertical ... 28/94\n",
      "Processing key azimuth_east_vertical ... 29/94\n",
      "Processing key elevation_hor_vertical ... 30/94\n",
      "Processing key Z_backward ... 31/94\n",
      "Processing key Z_L1_backward ... 32/94\n",
      "Processing key V_backward ... 33/94\n",
      "Processing key R_backward ... 34/94\n",
      "Processing key latitude_backward ... 35/94\n",
      "Processing key longitude_backward ... 36/94\n",
      "Processing key azimuth_east_backward ... 37/94\n",
      "Processing key elevation_hor_backward ... 38/94\n",
      "Processing key Z_transverse ... 39/94\n",
      "Processing key Z_L1_transverse ... 40/94\n",
      "Processing key V_transverse ... 41/94\n",
      "Processing key R_transverse ... 42/94\n",
      "Processing key latitude_transverse ... 43/94\n",
      "Processing key longitude_transverse ... 44/94\n",
      "Processing key azimuth_east_transverse ... 45/94\n",
      "Processing key elevation_hor_transverse ... 46/94\n",
      "Processing key distance_vertical_backward ... 47/94\n",
      "Processing key distance_vertical_tranverse ... 48/94\n",
      "Processing key Z ... 49/94\n",
      "Processing key Vx ... 50/94\n",
      "Processing key Vy ... 51/94\n",
      "Processing key Vz ... 52/94\n",
      "Processing key VE ... 53/94\n",
      "Processing key VN ... 54/94\n",
      "Processing key Mask_Vx ... 55/94\n",
      "Processing key Mask_Vy ... 56/94\n",
      "Processing key Mask_Vz ... 57/94\n",
      "Processing key Vx_error ... 58/94\n",
      "Processing key Vy_error ... 59/94\n",
      "Processing key Vz_error ... 60/94\n",
      "Processing key Temperature_field ... 61/94\n",
      "Processing key Pressure_field ... 62/94\n",
      "Processing key Mask_domain ... 63/94\n",
      "Processing key convective_index ... 64/94\n",
      "Processing key Mask_wind ... 65/94\n",
      "Processing key attenuation_phase_flag ... 66/94\n",
      "Processing key RainRate ... 67/94\n",
      "Processing key Gaseous_twowayatt ... 68/94\n",
      "Processing key w_ret ... 69/94\n",
      "Processing key iwc_ret ... 70/94\n",
      "Processing key Vt_ret ... 71/94\n",
      "Processing key Dm_ret ... 72/94\n",
      "Processing key N0_ret ... 73/94\n",
      "Processing key iwc_IWC_Z_T ... 74/94\n",
      "Processing key extinction_ret ... 75/94\n",
      "Processing key re_ret ... 76/94\n",
      "Processing key Nt_ret ... 77/94\n",
      "Processing key Z_in ... 78/94\n",
      "Processing key V_in ... 79/94\n",
      "Processing key T_in ... 80/94\n",
      "Processing key Z_fwd ... 81/94\n",
      "Processing key Z_noatt_fwd ... 82/94\n",
      "Processing key V_fwd ... 83/94\n",
      "Processing key Z_Xband ... 84/94\n",
      "Processing key error_v ... 85/94\n",
      "Processing key error_lnz ... 86/94\n",
      "Processing key lniwc_apriori ... 87/94\n",
      "Processing key error_lniwc_apriori ... 88/94\n",
      "Processing key Jd ... 89/94\n",
      "Processing key Ju ... 90/94\n",
      "Processing key iJd ... 91/94\n",
      "Processing key iJu ... 92/94\n",
      "Processing key lniwc_error ... 93/94\n",
      "Processing key w_error ... 94/94\n"
     ]
    }
   ],
   "source": [
    "# Destructively concatenate RASTA data to the merged dataset\n",
    "outputfile = 'tmp/darwin_rasta.nc'\n",
    "# Read RASTA files\n",
    "def read_rasta(filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    tmpflight = int(re.search('(?<=_F)\\d*(?=_radonvar)',filename).group())\n",
    "    rasta = xr.open_dataset(filepath)\n",
    "\n",
    "    sec = 3600*rasta.time.values\n",
    "    time = (pd.TimedeltaIndex(sec,unit='s') + pd.Timestamp(meta[tmpflight])).round('s')\n",
    "    rasta['time'] = time\n",
    "\n",
    "    if time.is_unique is False:\n",
    "        print('Warning!! Duplicate data in saffire is found, dropping ->')\n",
    "        print(sum(time.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        uniindex = np.where(~time.duplicated(keep='last'))[0]\n",
    "        rasta = rasta.isel(time=uniindex)\n",
    "    \n",
    "    rasta.rename({'time':'timeutc'},inplace=True)\n",
    "    return rasta\n",
    "\n",
    "rastafilenames = glob.glob('/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/RASTA/data/*.nc')\n",
    "\n",
    "rastaset = [ read_rasta(filepath) for filepath in rastafilenames ]\n",
    "timeutc = xr.open_dataset('tmp/darwin/darwin_sync_bulk.nc',group='/saffire').timeutc\n",
    "\n",
    "key = 'timeutc'\n",
    "xr.Dataset(data_vars={'timeutc':timeutc}).to_netcdf(outputfile,mode='w', ## 'w' for first time, later change to 'a'\n",
    "    format='NETCDF4',engine='netcdf4',group='/',unlimited_dims=['timeutc'])\n",
    "\n",
    "keys = list(rastaset[0].data_vars.keys())\n",
    "tot = len(keys)\n",
    "i = 0\n",
    "for key in keys:\n",
    "    i+=1\n",
    "    print('Processing key '+key+' ... '+str(i)+'/'+str(tot))\n",
    "    tmpset = [ x[key] for x in rastaset ]\n",
    "\n",
    "    datasettmp = xr.concat(tmpset,dim='timeutc')\n",
    "    datasettmp = xr.align(datasettmp,indexes={'timeutc':timeutc})[0]\n",
    "    datasettmp.to_netcdf(outputfile,mode='a', ## 'w' for first time, later change to 'a'\n",
    "        format='NETCDF4',engine='netcdf4',group='/'+key,unlimited_dims=['timeutc'],\n",
    "        encoding={ key:{'zlib':True, 'complevel':1} } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-05T21:34:11.406732Z",
     "start_time": "2017-06-05T21:34:08.649000Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct link table to MTSAT file\n",
    "dirpaths = glob.glob('/data/gpm/a/shared/szhu28/hiwcproc/darwin/mtsat/*')\n",
    "dirpaths.sort()\n",
    "\n",
    "allfiles = []\n",
    "for dirpath in dirpaths:\n",
    "    datestr = os.path.basename(dirpath)\n",
    "    filepaths = glob.glob(dirpath+'/*')\n",
    "    filepaths.sort()\n",
    "    for filepath in filepaths:\n",
    "        filename = os.path.basename(filepath)\n",
    "        timestr = os.path.splitext(filename)[0]\n",
    "        allfiles.append((datestr+timestr,filepath))\n",
    "\n",
    "timesat = pd.DatetimeIndex( [ x[0] for x in allfiles ] )\n",
    "satfiles = np.array( [ x[1] for x in allfiles ] )\n",
    "satfiles = xr.DataArray(satfiles, dims=['timesat'], coords={'timesat':timesat}, name='satfiles')\n",
    "\n",
    "sf = xr.open_dataset('tmp/darwin/darwin_sync_bulk.nc',group='/saffire')[ ['latitude','longitude'] ]\n",
    "timeutc = sf.timeutc.values\n",
    "orig = satfiles.timesat.values\n",
    "insind = np.searchsorted(orig,timeutc)\n",
    "torf = abs(timeutc-orig[insind-1])>abs(timeutc-orig[insind])\n",
    "inserted = np.where(torf, orig[insind], orig[insind-1])\n",
    "\n",
    "indarr = np.where(np.insert(diff(inserted),0,0))[0]\n",
    "indarr = np.concatenate([[0],indarr,[None]])\n",
    "\n",
    "varkeys = [\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'reflectance_vis',\n",
    " 'reflectance_nir',\n",
    " 'temperature_sir',\n",
    " 'temperature_67',\n",
    " 'temperature_ir',\n",
    " 'temperature_sw',\n",
    " 'broadband_shortwave_albedo',\n",
    " 'broadband_longwave_flux',\n",
    " 'ir_cloud_emittance',\n",
    " 'cloud_phase',\n",
    " 'visible_optical_depth',\n",
    " 'particle_size',\n",
    " 'liquid_water_path',\n",
    " 'cloud_effective_temperature',\n",
    " 'cloud_top_pressure',\n",
    " 'cloud_effective_pressure',\n",
    " 'cloud_bottom_pressure',\n",
    " 'cloud_top_height',\n",
    " 'cloud_effective_height',\n",
    " 'cloud_bottom_height']\n",
    "\n",
    "satds = xr.Dataset(data_vars={'timeutc':timeutc})\n",
    "\n",
    "def f_dist(x1,y1,x2,y2):\n",
    "    # remember to switch to radial before using\n",
    "    x1,y1,x2,y2 = [ f_rad(x) for x in [x1,y1,x2,y2] ]\n",
    "    return np.sqrt(((x1-x2)*np.cos((y1+y2)/2))**2 + (y1-y2)**2)\n",
    "def f_rad(x):\n",
    "    return x/180*np.pi\n",
    "\n",
    "def f_4ptinterp(xi,yi,xs,ys,zs):\n",
    "    dists = f_dist(xi,yi,xs,ys)\n",
    "    return np.average(zs,weights=np.minimum(1/dists,1e10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-05T21:53:09.118512Z",
     "start_time": "2017-06-05T21:38:42.308273Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# Process each MTSAT files and generate along-flight dataset\n",
    "toconcat = []\n",
    "outputfile = 'tmp/darwin_mtsat.nc'\n",
    "warnings.filterwarnings('ignore')\n",
    "for i in range(len(indarr)-1):\n",
    "    print(i)\n",
    "# for i in range(2):\n",
    "    indi = slice(indarr[i],indarr[i+1])\n",
    "    filepath = str(satfiles.sel(timesat=inserted[indarr[i]]).values)\n",
    "\n",
    "    # sadly in the original data file, the missing_value attribute is a string instead of a number, \n",
    "    # and thus the auto NAN identification of xarray doesn't work\n",
    "    tmpds = xr.open_dataset(filepath, mask_and_scale=False)\n",
    "    tmpds = tmpds[varkeys]\n",
    "\n",
    "    # longitude and latitude have different missing values\n",
    "    for key in varkeys[0:2]:\n",
    "        tmpds[key] = tmpds[key].where(~(tmpds[key]==-99999.))\n",
    "    for key in varkeys[2:]:\n",
    "        tmpds[key] = tmpds[key].where(~(tmpds[key]==-9.))\n",
    "\n",
    "    frameds = sf.isel(timeutc=indi)\n",
    "\n",
    "    lons = tmpds['longitude'].values\n",
    "    lats = tmpds['latitude'].values\n",
    "    lon = frameds.longitude.values\n",
    "    lat = frameds.latitude.values\n",
    "\n",
    "    # Generate two arrays for lon, lat\n",
    "    # 1. smallest value greater than all the elements before (inclusive) the current line (sgtb)\n",
    "    # 2. greatest value smaller than all the elements after (inclusive) the current line (gsta)\n",
    "\n",
    "    # For latitude, the generate trend is DECREASING wrt. index increasing.\n",
    "    tmp = np.nanmax(lons,axis=0)\n",
    "    tmp[isnan(tmp)] = -inf\n",
    "    tmp = np.maximum.accumulate(tmp)\n",
    "    tmpind = np.nonzero(tmp==-inf)[0][-1]\n",
    "    tmp[:tmpind+1] = tmp[tmpind+1]\n",
    "    sgtblon = tmp\n",
    "\n",
    "    tmp = np.nanmin(lons,axis=0)\n",
    "    tmp[isnan(tmp)] = inf\n",
    "    tmp = np.minimum.accumulate(tmp[::-1])[::-1]\n",
    "    tmpind = np.nonzero(tmp==inf)[0][0]\n",
    "    tmp[tmpind:] = tmp[tmpind-1]\n",
    "    gstalon = tmp\n",
    "\n",
    "    # any(gstalon>sgtblon) This should be false if the above codes work fine.\n",
    "\n",
    "    # For latitude, the generate trend is DECREASING wrt. index increasing.\n",
    "    tmp = np.nanmax(lats,axis=1)\n",
    "    tmp[isnan(tmp)] = -inf\n",
    "    tmp = np.maximum.accumulate(tmp[::-1])[::-1]\n",
    "    tmpind = np.nonzero(tmp==-inf)[0][0]\n",
    "    tmp[tmpind:] = tmp[tmpind-1]\n",
    "    sgtblat = tmp\n",
    "\n",
    "    tmp = np.nanmin(lats,axis=1)\n",
    "    tmp[isnan(tmp)] = inf\n",
    "    tmp = np.minimum.accumulate(tmp)\n",
    "    tmpind = np.nonzero(tmp==inf)[0][-1]\n",
    "    tmp[:tmpind+1] = tmp[tmpind+1]\n",
    "    gstalat = tmp\n",
    "    # any(gstalat>sgtblat) This should be false if the above codes work fine.\n",
    "\n",
    "    # gsta -> upper bound, sgtb -> lower bound\n",
    "    lonr = np.searchsorted(gstalon, lon)+1 # +1 is for the upper bound exclusive in python\n",
    "    lonl = np.searchsorted(sgtblon, lon)-1 # -1 is for considering all possibility\n",
    "\n",
    "    # gsta -> upper bound, sgtb -> lower bound\n",
    "    latl = len(gstalat)-np.searchsorted(gstalat[::-1], lat)-1 # -1 is for considering all possibility\n",
    "    latr = len(sgtblat)-np.searchsorted(sgtblat[::-1], lat)+1 # +1 is for the upper bound exclusive in python\n",
    "\n",
    "    # Add another wrapper for key values loop\n",
    "    toaddds = {}\n",
    "    for key in varkeys:\n",
    "        tmpzs = tmpds[key].values\n",
    "        zint = []\n",
    "        for j in range(len(lon)):\n",
    "            indsubgrid = slice(latl[j],latr[j]),slice(lonl[j],lonr[j])\n",
    "            x = lons[indsubgrid]\n",
    "            if len(x) == 0:\n",
    "                zint.append(nan)\n",
    "                continue\n",
    "            y = lats[indsubgrid]\n",
    "            z = tmpzs[indsubgrid]\n",
    "            indmin = np.unravel_index( ((x-lon[j])**2+(y-lat[j])**2).argmin(), x.shape)\n",
    "\n",
    "            indnine = slice(indmin[0]-1,indmin[0]+2),slice(indmin[1]-1,indmin[1]+2)\n",
    "            xs = x[indnine].ravel()\n",
    "            ys = y[indnine].ravel()\n",
    "            zs = z[indnine].ravel()\n",
    "            zint.append(f_4ptinterp(lon[j],lat[j],xs,ys,zs))\n",
    "        zint = np.array(zint)\n",
    "        toaddds[key] = xr.DataArray(zint, dims=['timeutc'], coords={'timeutc':frameds.timeutc},\n",
    "                                    attrs=tmpds[key].attrs, name=key)\n",
    "    toconcat.append(xr.Dataset(toaddds))\n",
    "\n",
    "warnings.filterwarnings('default') # restore default settings\n",
    "\n",
    "mtsatproc = xr.concat(toconcat,dim='timeutc')\n",
    "\n",
    "# Remember to add the variable for time difference and a mask\n",
    "tmpmtsatproc = xr.Dataset({},coords={'timeutc':timeutc})\n",
    "tmp = timeutc - inserted\n",
    "tmpmtsatproc['timelag'] = xr.DataArray(tmp, dims=['timeutc'], coords={'timeutc':timeutc})\n",
    "# Note the threshold is 30 min for Darwin and 15 min for Cayenne\n",
    "tmpmtsatproc['validlagmask'] = xr.DataArray( tmp <= np.timedelta64(30,'m') , dims=['timeutc'], coords={'timeutc':timeutc})\n",
    "mtsatproc = xr.merge([tmpmtsatproc,mtsatproc],join='left')\n",
    "mtsatproc.to_netcdf(outputfile,mode='w',\n",
    "    format='NETCDF4',engine='netcdf4',group='/mtsatproc',unlimited_dims=['timeutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert and create NCAR ECMWF archive\n",
    "import pygrib\n",
    "import resource\n",
    "\n",
    "# Parameters for Darwin\n",
    "filepaths = glob.glob('/data/gpm/a/shared/szhu28/hiwcproc/era_interim/2014**/*regn128sc*',recursive=True)\n",
    "kw = {'lat1':-20,'lat2':10,'lon1':110,'lon2':160}\n",
    "outputnoext = 'tmp/darwin/ecmwf'\n",
    "\n",
    "filename = '/net/san-b8-ib/data/gpm/a/shared/szhu28/hiwcproc/era_interim/201402/ei.oper.an.pl.regn128sc.2014020306'\n",
    "# with pygrib.open(filename) as grbs:\n",
    "grbs = pygrib.open(filename)\n",
    "grbs.rewind()\n",
    "varfullnames = {x.shortName:(x.name, x.units) for x in grbs}\n",
    "numofvar = len(varfullnames)\n",
    "grbs.rewind()\n",
    "varnames = [x.shortName for x in grbs[0:numofvar]]\n",
    "\n",
    "# [ (i,varnames[i],varfullnames[varnames[i]]) for i in range(numofvar) ]\n",
    "\n",
    "filename = '/net/san-b8-ib/data/gpm/a/shared/szhu28/hiwcproc/era_interim/201402/ei.oper.an.pl.regn128uv.2014020306'\n",
    "# with pygrib.open(filename) as grbs:\n",
    "grbsuv = pygrib.open(filename)\n",
    "grbsuv.rewind()\n",
    "varfullnamesuv = {x.shortName:(x.name, x.units) for x in grbsuv}\n",
    "numofvaruv = len(varfullnamesuv)\n",
    "grbsuv.rewind()\n",
    "varnamesuv = [x.shortName for x in grbsuv[0:numofvaruv]]\n",
    "\n",
    "# [ (i,varnamesuv[i],varfullnamesuv[varnamesuv[i]]) for i in range(numofvaruv) ]\n",
    "\n",
    "grbs.rewind()\n",
    "level = np.array([x.level for x in grbs[0::numofvar] ]).astype(float)\n",
    "\n",
    "key = 'level'\n",
    "dimlvl = xr.DataArray(level, dims=['level'], coords={'level':level}, attrs={\n",
    "    'units':'hPa'}, name=key)\n",
    "\n",
    "_dump, lats, lons = grbs[1].data(**kw)\n",
    "lat, lon = lats[:,0], lons[0,:]\n",
    "key = 'lat'\n",
    "dimlat = xr.DataArray(lat, dims=['lat'], coords={'lat':lat}, attrs={\n",
    "    'units':'degree north'}, name=key)\n",
    "key = 'lon'\n",
    "dimlon = xr.DataArray(lon, dims=['lon'], coords={'lon':lon}, attrs={\n",
    "    'units':'degree east'}, name=key)\n",
    "\n",
    "encoding = dict(zip([varfullnames[x][0] for x in varnames],[{'zlib':True, 'complevel':1}]*len(varnames)))\n",
    "encodinguv = dict(zip([varfullnamesuv[x][0] for x in varnamesuv],[{'zlib':True, 'complevel':1}]*len(varnamesuv)))\n",
    "encoding = {**encoding,**encodinguv}\n",
    "\n",
    "def f_read_model_pair(filepair):\n",
    "    grbs = pygrib.open(filepair[0])\n",
    "    grbsuv = pygrib.open(filepair[1])\n",
    "    timemod = str(grbs[1].dataDate)+\"{:04d}\".format(grbs[1].dataTime)\n",
    "    tmponefile = {}\n",
    "    for skey in varnames:\n",
    "        key = varfullnames[skey][0]\n",
    "        units = varfullnames[skey][1]\n",
    "        tmpdata = np.array([ x.data(**kw)[0] for x in grbs[varnames.index(skey)::numofvar] ])\n",
    "        tmponefile[key] = xr.DataArray(tmpdata, coords=[dimlvl,dimlat,dimlon], attrs={'units':units}, name=key)\n",
    "    for skey in varnamesuv:\n",
    "        key = varfullnamesuv[skey][0]\n",
    "        units = varfullnamesuv[skey][1]\n",
    "        tmpdata = np.array([ x.data(**kw)[0] for x in grbsuv[varnamesuv.index(skey)::numofvaruv] ])\n",
    "        tmponefile[key] = xr.DataArray(tmpdata, coords=[dimlvl,dimlat,dimlon], attrs={'units':units}, name=key)\n",
    "\n",
    "    onefile = xr.Dataset(tmponefile, coords={'level':dimlvl,'lat':dimlat,'lon':dimlon}, attrs={'timemod':timemod})\n",
    "    return onefile, timemod\n",
    "\n",
    "filepaths.sort()\n",
    "filepairs = [ ( x, re.sub('regn128sc','regn128uv',x) ) for x in filepaths ]\n",
    "\n",
    "dslist = []\n",
    "timestrlist = []\n",
    "i=0\n",
    "for x in filepairs:\n",
    "    print(i, resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "    i+=1\n",
    "    ds, timestr = f_read_model_pair(x)\n",
    "    dslist.append(ds)\n",
    "    timestrlist.append(timestr)\n",
    "    \n",
    "with open(outputnoext+'_tmpstore.p','bw') as f:\n",
    "    pickle.dump([dslist,timestrlist],f)\n",
    "    \n",
    "timemods = pd.DatetimeIndex(timestrlist)\n",
    "timemod = xr.DataArray(timemods, dims=['timemod'], coords={'timemod':timemods}, name='timemod')\n",
    "xr.concat(dslist,dim=timemod).to_netcdf(outputnoext+'.nc',mode='w',\n",
    "    format='NETCDF4',engine='netcdf4',group='/', encoding=encoding)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "atmos",
   "language": "python",
   "name": "atmos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
