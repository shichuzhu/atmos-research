{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob, re, os\n",
    "import scipy\n",
    "from scipy import io, optimize\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "## For debug mode\n",
    "from IPython.core.debugger import Tracer\n",
    "#Tracer()() #this one triggers the debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numofflts = 23\n",
    "rootpath = '/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     25,
     98,
     114,
     131
    ]
   },
   "outputs": [],
   "source": [
    "## For IKP2 V5 data\n",
    "def read_ikp2(filename):\n",
    "\n",
    "    tmp = pd.read_csv(filename,header=4,usecols=range(14),\n",
    "                      na_values={'SIAltm':99999.,\n",
    "                                 'SINShead':999.,\n",
    "                                 'SRHWVSS':999.,\n",
    "                                 'Swdir':999.,\n",
    "                                 'Swspd':999.,\n",
    "                                 'XKBZR5s':-999.})\n",
    "    tmptime = pd.to_timedelta(tmp['Stimech'])\n",
    "    tmptime = pd.to_datetime(datestr) + tmptime\n",
    "    tmp['Stimech'] = tmptime\n",
    "\n",
    "    tmp.set_index(['Stimech'],verify_integrity=False,inplace=True,drop=False)\n",
    "    tmp.index.name=None\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in IKP is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    return tmp\n",
    "\n",
    "## For SAFFIRE V4 data\n",
    "def read_saffire(filename):\n",
    "\n",
    "    '''\n",
    "    Things to note\n",
    "    1. The time is in seconds and can be non-sharp seconds.\n",
    "    '''\n",
    "    \n",
    "    ##     Tracer()() #this one triggers the debugger\n",
    "    varnames = [\n",
    "        'Timeinsecond',\n",
    "        'event_marker',\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'altitude',\n",
    "        'altitude',\n",
    "        'platform_roll_angle',\n",
    "        'platform_pitch_angle',\n",
    "        'platform_orientation',\n",
    "        'air_pressure',\n",
    "        'air_temperature',\n",
    "        'air_temperature',\n",
    "        'air_temperature',\n",
    "        'dew_point_temperature',\n",
    "        'relative_humidity',\n",
    "        'humidity_mixing_ratio',\n",
    "        'humidity_mixing_ratio',\n",
    "        'humidity_mixing_ratio',\n",
    "        'platform_speed_wrt_air',\n",
    "        'platform_acceleration',\n",
    "        'platform_course',\n",
    "        'platform_speed_wrt_ground',\n",
    "        'platform_course',\n",
    "        'platform_speed_wrt_ground',\n",
    "        'upward_platform_speed_wrt_ground',\n",
    "        'angle_of_attack',\n",
    "        'angle_of_sideslip',\n",
    "        'eastward_wind',\n",
    "        'northward_wind',\n",
    "        'upward_air_velocity',\n",
    "        'wind_from_direction',\n",
    "        'wind_speed',\n",
    "        'mic_msofreqice_rs_sync_1']\n",
    "\n",
    "    lookup = 'Warning : most measurements are not valid before take-off and after landing'\n",
    "    comments = []\n",
    "    with open(filename) as myFile:\n",
    "        for num, line in enumerate(myFile, 1):\n",
    "            if lookup in line:\n",
    "                skipline = num\n",
    "                break\n",
    "            if num>50:\n",
    "                comments.append(line)\n",
    "                \n",
    "    [comments.pop() for i in range(4)]\n",
    "    comments = ''.join(comments)\n",
    "    ### r\"\\s+\" refers to one or more occurences of whitespace, while r\"\\s*\" will match zero and would raise a warning\n",
    "    tmp = pd.read_csv(filename,skiprows=skipline,names=varnames,sep=r\"\\s+\",na_values=3.40282347e+38)\n",
    "    tmp['timeutc'] = pd.to_datetime(\n",
    "        pd.Series(np.array(round(tmp['Timeinsecond']), dtype='timedelta64[s]')))+\\\n",
    "        (pd.to_datetime(datestr)-pd.to_datetime('1970-01-01'))\n",
    "\n",
    "    tmp.set_index(['timeutc'],verify_integrity=False,inplace=True,drop=True)\n",
    "    tmp.index.name=None\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in saffire is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "\n",
    "    return tmp, comments\n",
    "\n",
    "## For Robust data\n",
    "def read_robust(filename):\n",
    "    ## Use explicit names, since the name in data file could  be inconsistent\n",
    "    varnames = ['GMT','TWC_robust']\n",
    "    tmp = pd.read_excel(filename,names=varnames)\n",
    "    tmp['timeutc'] = pd.DatetimeIndex(tmp['GMT']).round('1s')\n",
    "    tmp.set_index(['timeutc'],inplace=True,drop=True,verify_integrity=False)\n",
    "    tmp.index.name=None\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in Robust is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    return tmp\n",
    "\n",
    "## For PSD / MSD data\n",
    "def read_sd(filename):\n",
    "    \n",
    "    tmp = pd.read_csv(filename,header=0,sep=r\"\\s+\")\n",
    "    tmptime = pd.to_timedelta(tmp['-999'],unit='s')\n",
    "    tmptime = pd.to_datetime(datestr) + tmptime\n",
    "    tmp['-999'] = tmptime\n",
    "    tmp.set_index(['-999'],verify_integrity=False,inplace=True,drop=True)\n",
    "    tmp.index.name=None\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in IKP is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    return tmp\n",
    "\n",
    "## For counts data\n",
    "def read_count(filename):\n",
    "    \n",
    "    tmp = pd.read_csv(filename,header=None)\n",
    "    ### Use tmp[0] instead of tmp['0'] here because index type here is integer rather than string\n",
    "    tmptime = pd.to_timedelta(tmp[0],unit='s')\n",
    "    tmptime = pd.to_datetime(datestr) + tmptime\n",
    "    tmp[0] = tmptime\n",
    "    tmp.set_index([0],verify_integrity=False,inplace=True,drop=True)\n",
    "    tmp.index.name=None\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in IKP is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Create meta file information\n",
    "### For flight dates and SAFFIRE V4 files\n",
    "datapath = rootpath + '/data_src/data20161012/saffire'\n",
    "flist = np.array( sorted(glob.glob(datapath+'/F20*.txt')) )\n",
    "\n",
    "datestrs = pd.Series(np.empty(numofflts))\n",
    "saffirefn = pd.Series(np.empty(numofflts))\n",
    "\n",
    "for szi in range(numofflts):\n",
    "    fn = os.path.basename(flist[szi])\n",
    "    saffirefn[szi] = fn\n",
    "    tmpdate = re.search(r'(?<=v4_).*(?=_)',fn).group()\n",
    "    tmpdate = tmpdate[:4]+'-'+tmpdate[4:6]+'-'+tmpdate[6:]\n",
    "    datestrs[szi] = tmpdate\n",
    "\n",
    "fileinfo = pd.DataFrame(index=np.arange(1,24).astype(str))\n",
    "fileinfo['flightdate'] = np.array(datestrs)\n",
    "fileinfo['comments'] = NaN\n",
    "fileinfo['saffire'] = np.array(saffirefn)\n",
    "\n",
    "### For IKP V5 files\n",
    "datapath = rootpath + '/data_src/data20161012/ikp'\n",
    "flist = sorted( glob.glob(datapath+'/f20*.csv') )\n",
    "flist = [os.path.basename(x) for x in flist]\n",
    "flist.insert(4,None) # no data of ikp from flight 5\n",
    "fileinfo['ikp'] = flist\n",
    "\n",
    "### For Robust TWC\n",
    "datapath = rootpath + '/data_src/data20151203/dataHAIC'\n",
    "flist =  sorted(glob.glob(datapath+'/Robust_data_flt*.xls'))\n",
    "flist = [os.path.basename(x) for x in flist]\n",
    "fileinfo['robust'] = flist\n",
    "\n",
    "### For PSD\n",
    "\n",
    "### These 2 variables are used in the next cell as well.\n",
    "sdnamelist = ['psddmax','psddeq','msddmax','msddeq','psdly']\n",
    "countnamelist = ['2dscounts','pipcounts']\n",
    "\n",
    "datapath = rootpath + '/data_src/data20151203/dataHAIC'\n",
    "\n",
    "tmp = [os.path.basename(x) for x in sorted(glob.glob(datapath+r'/*F#*Composite_Dmax*.txt'))]\n",
    "tmp.insert(20,None) # no psd data from flight 21\n",
    "fileinfo[sdnamelist[0]] = tmp\n",
    "\n",
    "tmp = [os.path.basename(x) for x in sorted(glob.glob(datapath+r'/*F#*Composite_Deq*.txt'))]\n",
    "tmp.insert(20,None)\n",
    "fileinfo[sdnamelist[1]] = tmp\n",
    "\n",
    "tmp = [os.path.basename(x) for x in sorted(glob.glob(datapath+r'/*F#*MassSizeD_Dmax*.txt'))]\n",
    "tmp.insert(20,None)\n",
    "fileinfo[sdnamelist[2]] = tmp\n",
    "\n",
    "tmp = [os.path.basename(x) for x in sorted(glob.glob(datapath+r'/*F#*MassSizeD_Deq*.txt'))]\n",
    "tmp.insert(20,None)\n",
    "fileinfo[sdnamelist[3]] = tmp\n",
    "\n",
    "tmp = [os.path.basename(x) for x in sorted(glob.glob(datapath+r'/*F#*Composite_Ly*.txt'))]\n",
    "tmp.insert(20,None)\n",
    "fileinfo[sdnamelist[4]] = tmp\n",
    "\n",
    "### For counts\n",
    "datapath = rootpath + '/../counts_HIWC/Darwin/Darwin'\n",
    "tmp = [os.path.basename(x) for x in sorted(glob.glob(datapath+r'/*chx0_v*.csv'))]\n",
    "[tmp.insert(szi-1,None) for szi in [14,21,22] ]\n",
    "fileinfo[countnamelist[0]] = tmp\n",
    "tmp = [os.path.basename(x) for x in sorted(glob.glob(datapath+r'/*Comptage*.csv'))]\n",
    "tmp.insert(20,None)\n",
    "fileinfo[countnamelist[1]] = tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing flight 1...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140116_fs140001.txt\n",
      "Warning!! Duplicate data in Robust is found, dropping ->\n",
      "2\n",
      "Processing flight 2...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140116_fs140002.txt\n",
      "Warning!! Duplicate data in Robust is found, dropping ->\n",
      "1\n",
      "Processing flight 3...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140117_fs140003.txt\n",
      "Warning!! Duplicate data in Robust is found, dropping ->\n",
      "90\n",
      "Processing flight 4...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140118_fs140004.txt\n",
      "Processing flight 5...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140121_fs140005.txt\n",
      "Processing flight 6...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140123_fs140006.txt\n",
      "Processing flight 7...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140124_fs140007.txt\n",
      "Warning!! Duplicate data in Robust is found, dropping ->\n",
      "1\n",
      "Processing flight 8...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140127_fs140008.txt\n",
      "Warning!! Duplicate data in Robust is found, dropping ->\n",
      "2\n",
      "Processing flight 9...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140128_fs140009.txt\n",
      "Processing flight 10...\n",
      "Warning!! Duplicate data in IKP is found, dropping ->\n",
      "10\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140129_fs140010.txt\n",
      "Processing flight 11...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140130_fs140011.txt\n",
      "Processing flight 12...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140202_fs140012.txt\n",
      "Processing flight 13...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140203_fs140013.txt\n",
      "Processing flight 14...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140204_fs140014.txt\n",
      "Processing flight 15...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140205_fs140015.txt\n",
      "Warning!! Duplicate data in Robust is found, dropping ->\n",
      "2\n",
      "Processing flight 16...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140207_fs140016.txt\n",
      "Processing flight 17...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140208_fs140017.txt\n",
      "Processing flight 18...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140208_fs140018.txt\n",
      "Processing flight 19...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140209_fs140019.txt\n",
      "Processing flight 20...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140210_fs140020.txt\n",
      "Processing flight 21...\n",
      "Warning!! Duplicate data in saffire is found, dropping ->\n",
      "1\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140217_fs140021.txt\n",
      "Missing PSD file\n",
      "Missing PSD file\n",
      "Missing PSD file\n",
      "Missing PSD file\n",
      "Missing PSD file\n",
      "Processing flight 22...\n",
      "Warning!! Duplicate data in saffire is found, dropping ->\n",
      "1\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140217_fs140022.txt\n",
      "Processing flight 23...\n",
      "/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/data_src/data20161012/saffire/F20_1Hz-HAIC_base_aipov_v4_20140218_fs140023.txt\n"
     ]
    }
   ],
   "source": [
    "## Create rawvar and rawpsd variables\n",
    "rawvar = pd.DataFrame()\n",
    "rawpsd = pd.DataFrame()\n",
    "for flt in range(1,numofflts+1):\n",
    "# for flt in range(1,1+1):\n",
    "    print('Processing flight '+str(flt)+'...')\n",
    "    filenames = fileinfo.loc[str(flt),:]\n",
    "    datestr = filenames['flightdate'] # Global function that will be used in the read file functions\n",
    "    ### IKP\n",
    "    datapath = rootpath + '/data_src/data20161012/ikp'\n",
    "    filename = datapath + '/' + str(filenames['ikp'])\n",
    "    if os.path.isfile(filename):\n",
    "        tmp1 = read_ikp2(filename)\n",
    "    else:\n",
    "        tmp1 = None\n",
    "\n",
    "    ### SAFFIRE\n",
    "    datapath = rootpath + '/data_src/data20161012/saffire'\n",
    "    filename = datapath + '/' + str(filenames['saffire'])\n",
    "    if os.path.isfile(filename):\n",
    "        tmp2, comments = read_saffire(filename)\n",
    "        fileinfo.loc[str(flt),'comments'] = comments\n",
    "    else:\n",
    "        tmp2 = None\n",
    "\n",
    "    ### Robust\n",
    "    datapath = rootpath + '/data_src/data20151203/dataHAIC'\n",
    "    filename = datapath + '/' + str(filenames['robust'])\n",
    "    if os.path.isfile(filename):\n",
    "        tmp3 = read_robust(filename)\n",
    "        fileinfo.loc[str(flt),'comments'] = comments\n",
    "    else:\n",
    "        tmp3 = None\n",
    "\n",
    "    ### SD\n",
    "    datapath = rootpath + '/data_src/data20151203/dataHAIC'\n",
    "    tmpsd = []\n",
    "    for varname in sdnamelist:\n",
    "        filename = datapath + '/' + str(filenames[varname])\n",
    "        if os.path.isfile(filename):\n",
    "            tmpsd.append(read_sd( filename ))\n",
    "        else:\n",
    "            print('Missing PSD file')\n",
    "            tmpsd.append(None)\n",
    "    ### counts\n",
    "    ### counts are omitted for now because it's in 1-sec resolution. Need future work.\n",
    "    # datapath = rootpath + '/../counts_HIWC/Darwin/Darwin'\n",
    "    \n",
    "    result = pd.concat([tmp1, tmp2, tmp3], axis=1,verify_integrity=True)\n",
    "    ### Drop the first line where all data is NaT or NaN, this must be done before creating indpsdbackward and flightnum\n",
    "    result.dropna(axis=0,how='all',inplace=True)\n",
    "    result['flightnum'] = flt\n",
    "    rawvar = pd.concat([rawvar,result],verify_integrity=True,axis=0)\n",
    "    if all([x is None for x in tmpsd]):\n",
    "        sdresult = None\n",
    "    else:\n",
    "        sdresult = pd.concat(tmpsd, axis=1,verify_integrity=True,keys=sdnamelist)\n",
    "    rawpsd = pd.concat([rawpsd,sdresult], axis=0,verify_integrity=True)\n",
    "    \n",
    "### combine sd after all flights are processed!\n",
    "### Create indpsdforward\n",
    "tmpind = rawpsd.shape\n",
    "rawvar = pd.concat([rawvar,pd.Series(np.arange(tmpind[0]),index=rawpsd.index,\n",
    "                                         name='indpsdforward')],axis=1,verify_integrity=True)\n",
    "\n",
    "### Create indpsdbackward\n",
    "tmpshp = rawvar.shape\n",
    "tmpshp = tmpshp[0]\n",
    "tmpseries = np.arange(tmpshp)\n",
    "indpsdforward = rawvar['indpsdforward'].as_matrix()\n",
    "indpsdbackward = tmpseries[~isnan(indpsdforward)]\n",
    "rawpsd['indpsdbackward'] = indpsdbackward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Save the created data to hdf5 file\n",
    "savekw = {'complib':None,'complevel':0,'format':'fixed'}\n",
    "## Surpressed for data conservation\n",
    "# rawvar.to_hdf('hiwcdata.h5',key='rawvar',**savekw)\n",
    "## save another rawvar as version hdf5 / netcdf4 using xarray.DataSet\n",
    "# rawvar.index.name='time'\n",
    "# xr.Dataset(rawvar).to_netcdf('pythondata/rawvards.h5',format='NETCDF4',mode='w')\n",
    "\n",
    "# rawpsd.to_hdf('hiwcdata.h5',key='rawpsd',**savekw)\n",
    "bin_div = np.arange(10.,12850.1,10.)\n",
    "pd.DataFrame(bin_div).to_hdf('hiwcdata.h5',key='bin_div')\n",
    "### rawpsd = pd.read_hdf('hiwcdata.h5',key='rawpsd') # For read in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "### IGF routine\n",
    "f_mygamma = lambda nml, x: 10**nml[0]*x**nml[1]*np.exp(-nml[2]*x)\n",
    "def f_one_mode(psd, bin_div, moments):\n",
    "    bin_diff = np.diff(bin_div)\n",
    "    bin_mid = (bin_div[:-1]+bin_div[1:])/2.\n",
    "    mobs = np.empty(moments.shape)\n",
    "    for szmoment in range(len(mobs)):\n",
    "        mobs[szmoment] = np.sum( psd*bin_diff*bin_mid**(moments[szmoment]) )\n",
    "\n",
    "    x0 = np.array([log10(300), -1, 0.0014])\n",
    "    tmpind = psd>0\n",
    "    tmpfun = lambda x, nml1, nml2, nml3: f_mygamma(np.array([nml1,nml2,nml3]),x)\n",
    "    nml0, pcov = scipy.optimize.curve_fit(tmpfun, bin_mid[tmpind], psd[tmpind], p0=x0)\n",
    "    # nml0 = [1.8744, -0.6307, 0.0033]\n",
    "    f_fit = lambda nml: f_sum_chisquare(nml, moments, bin_mid, bin_diff, mobs)\n",
    "    optresult = scipy.optimize.minimize(f_fit, nml0, method='Nelder-Mead')\n",
    "    return optresult\n",
    "    \n",
    "def f_sum_chisquare( nml, moments, bin_mid, bin_diff, mobs ):\n",
    "    psd = f_mygamma(nml, bin_mid)\n",
    "    ### We may drop this condition if the code works fine.\n",
    "#     if any(np.isnan(psd)) or any(np.isinf(psd)):\n",
    "#         return np.inf\n",
    "    mfit = np.empty(mobs.shape)\n",
    "    for szmoment in range(len(mfit)):\n",
    "        mfit[szmoment] = np.sum( psd*bin_diff*bin_mid**(moments[szmoment]) )\n",
    "    return np.sum( (mfit-mobs)**2/mobs/mfit )\n",
    "\n",
    "## For median mass diameter calculation\n",
    "def f_mmd(bin_div, msd):\n",
    "    bin_diff = np.diff(bin_div)\n",
    "    cmsd = np.concatenate( (np.array([0]),np.cumsum(msd*bin_diff)) )\n",
    "    if cmsd[-1]<=0:\n",
    "        return np.NaN\n",
    "    cmsd /= cmsd[-1]\n",
    "    indtmp = np.where(np.diff(cmsd>0.5)==1)[0]\n",
    "    x1,x2,y1,y2 = bin_div[indtmp], bin_div[indtmp+1], cmsd[indtmp], cmsd[indtmp+1]\n",
    "    mmd = (x2-x1)/(y2-y1)*(0.5-y1)+x1\n",
    "    return mmd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0 ... done\r",
      "1 ... N/A\r",
      "2 ... N/A\r",
      "3 ... N/A\r",
      "4 ... N/A\r",
      "5 ... N/A\r",
      "6 ... N/A\r",
      "7 ... N/A\r",
      "8 ... N/A\r",
      "9 ... N/A\r",
      "10 ... N/A\r",
      "11 ... N/A\r",
      "12 ... N/A\r",
      "13 ... N/A\r",
      "14 ... N/A\r",
      "15 ... N/A\r",
      "16 ... N/A\r",
      "17 ... N/A\r",
      "18 ... N/A\r",
      "19 ... N/A\r",
      "20 ... N/A\r",
      "21 ... N/A\r",
      "22 ... N/A\r",
      "23 ... N/A\r",
      "24 ... N/A\r",
      "25 ... N/A\r",
      "26 ... N/A\r",
      "27 ... N/A\r",
      "28 ... N/A\r",
      "29 ... N/A\r",
      "30 ... N/A\r",
      "31 ... N/A\r",
      "32 ... N/A\r",
      "33 ... N/A\r",
      "34 ... N/A\r",
      "35 ... N/A\r",
      "36 ... N/A\r",
      "37 ... N/A\r",
      "38 ... N/A\r",
      "39 ... N/A\r",
      "40 ... N/A\r",
      "41 ... N/A\r",
      "42 ... N/A\r",
      "43 ... N/A\r",
      "44 ... N/A\r",
      "45 ... N/A\r",
      "46 ... N/A\r",
      "47 ... N/A\r",
      "48 ... N/A\r",
      "49 ... N/A\r",
      "50 ... N/A\r",
      "51 ... N/A\r",
      "52 ... N/A\r",
      "53 ... N/A\r",
      "54 ... N/A\r",
      "55 ... N/A\r",
      "56 ... N/A\r",
      "57 ... N/A\r",
      "58 ... N/A\r",
      "59 ... N/A\r",
      "60 ... N/A\r",
      "61 ... N/A\r",
      "62 ... N/A\r",
      "63 ... N/A\r",
      "64 ... N/A\r",
      "65 ... N/A\r",
      "66 ... N/A\r",
      "67 ... N/A\r",
      "68 ... N/A\r",
      "69 ... N/A\r",
      "70 ... N/A\r",
      "71 ... N/A\r",
      "72 ... N/A\r",
      "73 ... N/A\r",
      "74 ... N/A\r",
      "75 ... N/A\r",
      "76 ... N/A\r",
      "77 ... N/A\r",
      "78 ... N/A\r",
      "79 ... N/A\r",
      "80 ... N/A\r",
      "81 ... N/A\r",
      "82 ... N/A\r",
      "83 ... N/A\r",
      "84 ... N/A\r",
      "85 ... N/A\r",
      "86 ... N/A\r",
      "87 ... N/A\r",
      "88 ... N/A\r",
      "89 ... N/A\r",
      "90 ... N/A\r",
      "91 ... N/A\r",
      "92 ... N/A\r",
      "93 ... N/A\r",
      "94 ... N/A\r",
      "95 ... N/A\r",
      "96 ... N/A\r",
      "97 ... N/A\r",
      "98 ... N/A\r",
      "99 ... N/A\r",
      "100 ... N/A\r",
      "101 ... N/A\r",
      "102 ... N/A\r",
      "103 ... N/A\r",
      "104 ... N/A\r",
      "105 ... N/A\r",
      "106 ... N/A\r",
      "107 ... N/A\r",
      "108 ... N/A\r",
      "109 ... N/A\r",
      "110 ... N/A\r",
      "111 ... N/A\r",
      "112 ... N/A\r",
      "113 ... N/A\r",
      "114 ... N/A\r",
      "115 ... N/A\r",
      "116 ... N/A\r",
      "117 ... N/A\r",
      "118 ... N/A\r",
      "119 ... N/A\r",
      "120 ... N/A\r",
      "121 ... N/A\r",
      "122 ... N/A\r",
      "123 ... N/A\r",
      "124 ... N/A\r",
      "125 ... N/A\r",
      "126 ... N/A\r",
      "127 ... N/A\r",
      "128 ... N/A\r",
      "129 ... N/A\r",
      "130 ... N/A\r",
      "131 ... N/A\r",
      "132 ... N/A\r",
      "133 ... N/A\r",
      "134 ... N/A\r",
      "135 ... N/A\r",
      "136 ... N/A\r",
      "137 ... N/A\r",
      "138 ... N/A\r",
      "139 ... N/A\r",
      "140 ... N/A\r",
      "141 ... N/A\r",
      "142 ... N/A\r",
      "143 ... N/A\r",
      "144 ... N/A\r",
      "145 ... N/A\r",
      "146 ... N/A\r",
      "147 ... N/A\r",
      "148 ... N/A\r",
      "149 ... N/A\r",
      "150 ... N/A\r",
      "151 ... N/A\r",
      "152 ... N/A\r",
      "153 ... N/A\r",
      "154 ... N/A\r",
      "155 ... N/A\r",
      "156 ... N/A\r",
      "157 ... N/A\r",
      "158 ... N/A\r",
      "159 ... N/A\r",
      "160 ... N/A\r",
      "161 ... N/A\r",
      "162 ... N/A\r",
      "163 ... N/A\r",
      "164 ... N/A\r",
      "165 ... N/A\r",
      "166 ... N/A\r",
      "167 ... N/A\r",
      "168 ... N/A\r",
      "169 ... N/A\r",
      "170 ... N/A\r",
      "171 ... N/A\r",
      "172 ... N/A\r",
      "173 ... N/A\r",
      "174 ... N/A\r",
      "175 ... N/A\r",
      "176 ... N/A\r",
      "177 ... N/A\r",
      "178 ... N/A\r",
      "179 ... N/A\r",
      "180 ... N/A\r",
      "181 ... N/A\r",
      "182 ... N/A\r",
      "183 ... N/A\r",
      "184 ... N/A\r",
      "185 ... N/A\r",
      "186 ... N/A\r",
      "187 ... N/A\r",
      "188 ... N/A\r",
      "189 ... N/A\r",
      "190 ... N/A\r",
      "191 ... N/A\r",
      "192 ... N/A\r",
      "193 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/keeling/a/szhu28/usr/anaconda2/envs/py3k/lib/python3.5/site-packages/ipykernel/__main__.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/keeling/a/szhu28/usr/anaconda2/envs/py3k/lib/python3.5/site-packages/ipykernel/__main__.py:27: RuntimeWarning: overflow encountered in square\n",
      "/data/keeling/a/szhu28/usr/anaconda2/envs/py3k/lib/python3.5/site-packages/scipy/optimize/optimize.py:528: RuntimeWarning: invalid value encountered in subtract\n",
      "  numpy.max(numpy.abs(fsim[0] - fsim[1:])) <= fatol):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/keeling/a/szhu28/usr/anaconda2/envs/py3k/lib/python3.5/site-packages/ipykernel/__main__.py:27: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/keeling/a/szhu28/usr/anaconda2/envs/py3k/lib/python3.5/site-packages/ipykernel/__main__.py:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/keeling/a/szhu28/usr/anaconda2/envs/py3k/lib/python3.5/site-packages/ipykernel/__main__.py:2: RuntimeWarning: overflow encountered in power\n",
      "  from ipykernel import kernelapp as app\n",
      "/data/keeling/a/szhu28/usr/anaconda2/envs/py3k/lib/python3.5/site-packages/ipykernel/__main__.py:2: RuntimeWarning: overflow encountered in multiply\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201 ... Error encountered\n",
      "1215 ... Error encountered\n",
      "1217 ... Error encountered\n",
      "1218 ... Error encountered\n",
      "1239 ... Error encountered\n",
      "1240 ... Error encountered\n",
      "1242 ... Error encountered\n",
      "1245 ... Error encountered\n",
      "1248 ... Error encountered\n",
      "1249 ... Error encountered\n",
      "1253 ... Error encountered\n",
      "1255 ... Error encountered\n",
      "1256 ... Error encountered\n",
      "1260 ... Error encountered\n",
      "1269 ... Error encountered\n",
      "1270 ... Error encountered\n",
      "1312 ... Error encountered\n",
      "1317 ... Error encountered\n",
      "1326 ... Error encountered\n",
      "1327 ... Error encountered\n",
      "1328 ... Error encountered\n",
      "1331 ... Error encountered\n",
      "1332 ... Error encountered\n",
      "1338 ... Error encountered\n",
      "1346 ... Error encountered\n",
      "1347 ... Error encountered\n",
      "1360 ... Error encountered\n",
      "1365 ... Error encountered\n",
      "1367 ... Error encountered\n",
      "1368 ... Error encountered\n",
      "1371 ... Error encountered\n",
      "1373 ... Error encountered\n",
      "1380 ... Error encountered\n",
      "1381 ... Error encountered\n",
      "1383 ... Error encountered\n",
      "1384 ... Error encountered\n",
      "1390 ... Error encountered\n",
      "1400 ... Error encountered\n",
      "1406 ... Error encountered\n",
      "1407 ... Error encountered\n",
      "1498 ... Error encountered\n",
      "1507 ... Error encountered\n",
      "1509 ... Error encountered\n",
      "1550 ... Error encountered\n",
      "1552 ... Error encountered\n",
      "1553 ... Error encountered\n",
      "1556 ... Error encountered\n",
      "1560 ... Error encountered\n",
      "1561 ... Error encountered\n",
      "1567 ... Error encountered\n",
      "15796 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/keeling/a/szhu28/usr/anaconda2/envs/py3k/lib/python3.5/site-packages/ipykernel/__main__.py:26: RuntimeWarning: overflow encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25793 ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/keeling/a/szhu28/usr/anaconda2/envs/py3k/lib/python3.5/site-packages/scipy/optimize/minpack.py:715: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40214 ... doneOutput file saved\n"
     ]
    }
   ],
   "source": [
    "## IGF fit and save the data\n",
    "moments = np.array([0,2,3])\n",
    "PSDs = rawpsd['psddmax'].as_matrix()\n",
    "shp = PSDs.shape\n",
    "\n",
    "output = [None]*shp[0]\n",
    "for szi in range(shp[0]):\n",
    "# for szi in range(6):\n",
    "    psd = PSDs[szi,:]\n",
    "    print('\\r'+str(szi)+' ... ',end=\"\")\n",
    "    if np.sum(psd>0)>10:\n",
    "        try:\n",
    "            output[szi]=f_one_mode(psd,bin_div,moments)\n",
    "            print('done',end=\"\")\n",
    "        except:\n",
    "            output[szi]='Error'\n",
    "            print('Error encountered')\n",
    "    else:\n",
    "        print('N/A',end=\"\")\n",
    "\n",
    "file = open('output.p', 'wb')\n",
    "pickle.dump(output,file)\n",
    "file.close()\n",
    "print('Output file saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Compute MMD\n",
    "MSDs = rawpsd['msddmax'].as_matrix()\n",
    "shp = MSDs.shape\n",
    "MMD = np.empty(shp[0])\n",
    "for szi in range(shp[0]):\n",
    "# for szi in range(1):\n",
    "#     msd = MSDs[20066,:]\n",
    "    msd = MSDs[szi,:]\n",
    "    MMD[szi] = f_mmd(bin_div, msd)\n",
    "\n",
    "## Surpressed for data conservation\n",
    "# file = open('mmd.p', 'wb')\n",
    "# pickle.dump(MMD,file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Load the created data\n",
    "rawvar = pd.read_hdf('pythondata/hiwcdata.h5',key='rawvar')\n",
    "rawvards = xr.open_dataset('pythondata/rawvards.h5')\n",
    "rawpsd = pd.read_hdf('pythondata/hiwcdata.h5',key='rawpsd')\n",
    "bin_div = pd.read_hdf('pythondata/hiwcdata.h5',key='bin_div').as_matrix().ravel()\n",
    "\n",
    "with open('pythondata/mmd.p', 'rb') as file:\n",
    "    MMD = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform psd data from pandas to xarray\n",
    "bin_mid = (bin_div[1:]+bin_div[:-1])/2.\n",
    "rawpsd.index.rename('time',inplace=True)\n",
    "tmpda = []\n",
    "for psdstr in rawpsd.keys().levels[0][:-1]:\n",
    "    a=xr.DataArray(rawpsd[psdstr],coords=[('time',rawpsd.index),('bin',bin_mid)],name=psdstr)\n",
    "    tmpda.append(a)\n",
    "lastkey = rawpsd.keys().levels[0][-1]\n",
    "a=xr.DataArray(rawpsd[lastkey],coords=[('time',rawpsd.index)],name=lastkey)\n",
    "tmpda.append(a)\n",
    "psdds = xr.merge(tmpda)\n",
    "\n",
    "psdds.to_netcdf('pythondata/psdds.h5',format='NETCDF4',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-09 23:05:08.926636\n",
      "Reading rasta raw files ...\n",
      "flight 1 good\n",
      "flight 2 good\n",
      "flight 3 good\n",
      "flight 4 good\n",
      "flight 6 good\n",
      "flight 7 good\n",
      "flight 8 good\n",
      "flight 9 good\n",
      "flight 10 good\n",
      "flight 11 good\n",
      "flight 12 good\n",
      "flight 13 good\n",
      "flight 14 good\n",
      "flight 15 good\n",
      "flight 16 good\n",
      "flight 17 good\n",
      "flight 19 good\n",
      "flight 20 good\n",
      "flight 21 good\n",
      "flight 22 good\n",
      "flight 23 good\n",
      "2016-12-09 23:05:15.650975\n",
      "Finished reading, combining ...\n"
     ]
    }
   ],
   "source": [
    "## combining rawvar with rasta and create large rastacombine.h5 file\n",
    "def loadRastaflt(szi):\n",
    "    datapath='/data/mcfarq/a/szhu28/research/HIWC/data/fulldataDarwin/RASTA/data/'\n",
    "    rastafn=glob.glob(datapath+'*_F'+str(szi)+'_*.nc')\n",
    "    if len(rastafn) ==0:\n",
    "        return None\n",
    "    ds = xr.open_dataset(rastafn[0])\n",
    "    tmp = (ds.time.values*3600).astype('timedelta64[s]')\n",
    "    midnight = np.datetime64(rawvar.index[rawvar['flightnum']==szi][0].date())\n",
    "    ds['timeSec'] = ds.time\n",
    "    ds['time'] = xr.DataArray(tmp+midnight,coords={'time':ds.time})\n",
    "    tmp = ds.time\n",
    "    if tmp.shape == np.unique(tmp).shape:\n",
    "        print('flight '+str(szi)+' good')\n",
    "    else:\n",
    "        print(str(tmp.shape - np.unique(tmp).shape)+' data duplicate disregarded in the future')\n",
    "    return ds\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "print('Reading rasta raw files ...')\n",
    "tmplist = [loadRastaflt(szi) for szi in range(1,24)]\n",
    "print(datetime.datetime.now())\n",
    "print('Finished reading, combining ...')\n",
    "# list(filter((None).__ne__, tmplist)) is a method to remove all the None cases in the list\n",
    "rastadataset = xr.concat(list(filter((None).__ne__, tmplist)),dim='time')\n",
    "print(datetime.datetime.now())\n",
    "print('Writing to file ...')\n",
    "rastadataset.to_netcdf('pythondata/rastacombine.h5',format='NETCDF4',mode='w')\n",
    "print('Done.')\n",
    "print(datetime.datetime.now())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
