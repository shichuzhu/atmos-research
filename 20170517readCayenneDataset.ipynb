{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T03:41:16.176081Z",
     "start_time": "2017-06-16T03:40:49.615Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob, re, os\n",
    "import scipy\n",
    "from scipy import io, optimize\n",
    "import pickle\n",
    "import datetime\n",
    "import netCDF4\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings('default') # restore default settings\n",
    "\n",
    "## For debug mode\n",
    "from IPython.core.debugger import Tracer\n",
    "#Tracer()() #this one triggers the debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-05T21:50:15.920646Z",
     "start_time": "2017-06-05T21:50:15.888760Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# meta data for defining time range of flights\n",
    "metatmp = [\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150509_fs150009_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150510_fs150010_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150512_fs150011_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150514_fs150012_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150515_fs150013_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150516_fs150014_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150516_fs150015_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150518_fs150016_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150519_fs150017_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150523_fs150018_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150523_fs150019_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150524_fs150020_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150525_fs150021_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150526_fs150022_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150526_fs150023_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150527_fs150024_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150528_fs150025_20151106_V1.txt',\n",
    "    'F20_1Hz-HAIC-2015_core_v4_20150529_fs150026_20151106_V1.txt']\n",
    "    #                          26,    34,     41,43\n",
    "# meta = [(tmp[26:34], int(tmp[41:43])) for tmp in metatmp]\n",
    "meta = {int(tmp[41:43]):tmp[26:34] for tmp in metatmp}\n",
    "\n",
    "outputfile = ''\n",
    "# cayenne_bulk.nc  required for generating time-sync files\n",
    "# cayenne_sync.nc  huge time sync file containing all data\n",
    "# cayenne_sync_bulk.nc  all data except lamp psd\n",
    "# cayenne_sync_gz.nc  zlib compressed lamp psd only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T14:10:40.734712Z",
     "start_time": "2017-05-15T14:10:36.732848Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## For SAFFIRE V4 data\n",
    "def read_saffire(filepath):\n",
    "    tmp = os.path.basename(filepath)\n",
    "    tmpflight = int(tmp[41:43])\n",
    "    tmpdate = meta[tmpflight]\n",
    "    '''\n",
    "    Things to note\n",
    "    1. The time is in seconds and can be non-sharp seconds.\n",
    "    '''\n",
    "    varnames = [\n",
    "        'Timeinsecond',\n",
    "        'event_marker',\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'altitude_gps',\n",
    "        'altitude_airins',\n",
    "        'platform_roll_angle',\n",
    "        'platform_pitch_angle',\n",
    "        'platform_orientation',\n",
    "        'air_pressure',\n",
    "        'air_temperature_rm',\n",
    "        'air_temperature_impact',\n",
    "        'air_temperature_adc',\n",
    "        'dew_point_temperature',\n",
    "        'relative_humidity',\n",
    "        'humidity_mixing_ratio_hygrometer',\n",
    "        'humidity_mixing_ratio_aerodata',\n",
    "        'humidity_mixing_ratio_wvss2',\n",
    "        'platform_speed_wrt_air',\n",
    "        'platform_acceleration',\n",
    "        'platform_course',\n",
    "        'platform_speed_wrt_ground_aipov',\n",
    "        'platform_course',\n",
    "        'platform_speed_wrt_ground_gps',\n",
    "        'upward_platform_speed_wrt_ground',\n",
    "        'angle_of_attack',\n",
    "        'angle_of_sideslip',\n",
    "        'eastward_wind',\n",
    "        'northward_wind',\n",
    "        'upward_air_velocity',\n",
    "        'wind_from_direction',\n",
    "        'wind_speed',\n",
    "        'mic_msofreqice_rs_sync_1'\n",
    "    ]\n",
    "\n",
    "    lookup = 'Warning : most measurements are not valid before take-off and after landing'\n",
    "    comments = []\n",
    "    with open(filepath) as myFile:\n",
    "        for num, line in enumerate(myFile, 1):\n",
    "            if lookup in line:\n",
    "                skipline = num\n",
    "                break\n",
    "    ### r\"\\s+\" refers to one or more occurences of whitespace, while r\"\\s*\" will match zero and would raise a warning\n",
    "    tmp = pd.read_csv(filepath,skiprows=skipline,names=varnames,sep=r\"\\s+\",na_values=3.40282347e+38)\n",
    "\n",
    "    # pd.TimedeltaIndex(round(tmp.Timeinsecond).astype(int),units='s')\n",
    "    tmp.loc[:,'time'] = (pd.TimedeltaIndex(tmp.Timeinsecond,unit='s') + pd.Timestamp(tmpdate)).round('s')\n",
    "    tmp.set_index(keys='time',inplace=True,verify_integrity=False)\n",
    "    tmp = tmp[pd.notnull(tmp.index)]\n",
    "    tmp.index.rename('timeutc',inplace=True)\n",
    "\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in saffire is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    tmp['flightnum'] = tmpflight\n",
    "    return tmp\n",
    "\n",
    "saffireset = [ read_saffire(filepath) for filepath \n",
    "          in glob.glob('/data/mcfarq/a/szhu28/research/HIWC/data/fulldataCayenne/20170513lamp/saffire/*') ]\n",
    "# astype does not provide a 'coerce' option\n",
    "datasetsaffire = pd.DataFrame().append(saffireset)\n",
    "datasetsaffire.to_xarray().to_netcdf(outputfile,mode='a',\n",
    "    format='NETCDF4',engine='netcdf4',group='/saffire',unlimited_dims=['timeutc'])\n",
    "# datasetsaffire.apply(pd.to_numeric,errors='raise').dropna(how='all').equals(datasetsaffire) -> gives a True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T14:10:43.211107Z",
     "start_time": "2017-05-15T14:10:40.739753Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read IKP\n",
    "# with open('tmp/ikpdata.p','rb') as fin:\n",
    "#     datasetipk = pickle.load(fin)\n",
    "\n",
    "def read_ikp2(filepath):\n",
    "    tmp = os.path.basename(filepath)\n",
    "    tmpdate, tmpflight = tmp[4:8]+tmp[9:11]+tmp[12:14],int(tmp[23:25])\n",
    "    # Skip the first row of each csv file. Warning this would lose data! Otherwise data are read as string, not float\n",
    "    # tmp = pd.read_csv(filepath, header=4, skiprows=1, na_values=-999.0)\n",
    "    tmp = pd.read_csv(filepath, header=4, na_values=-999.0)\n",
    "    tmp.loc[:,'time'] = pd.Timestamp(tmpdate) + pd.to_timedelta(tmp['time'],errors='coerce') # Get rid of null lines\n",
    "    tmp.set_index(keys='time',inplace=True,verify_integrity=False)\n",
    "    tmp = tmp[pd.notnull(tmp.index)]\n",
    "    tmp.index.rename('timeutc',inplace=True)\n",
    "\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in IKP is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    return tmp\n",
    "\n",
    "ikpset = [ read_ikp2(filepath) for filepath \n",
    "          in glob.glob('/data/mcfarq/a/szhu28/research/HIWC/data/fulldataCayenne/20170513lamp/ikp/*') ]\n",
    "\n",
    "datasetipk = pd.DataFrame().append(ikpset).astype(float).replace(-999., nan)\n",
    "datasetipk.to_xarray().to_netcdf(outputfile,mode='a', ## 'w' for first time, later change to 'a'\n",
    "    format='NETCDF4',engine='netcdf4',group='/ikp',unlimited_dims=['timeutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T14:10:44.876090Z",
     "start_time": "2017-05-15T14:10:43.217105Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read robust\n",
    "def read_robust(filepath):\n",
    "    global meta\n",
    "    tmp = os.path.basename(filepath)\n",
    "\n",
    "    tmpflight = int(tmp[33:35])\n",
    "    tmpdate = meta[tmpflight]\n",
    "    # Skip the first row of each csv file. Warning this would lose data! Otherwise data are read as string, not float\n",
    "    # tmp = pd.read_csv(filepath, header=4, skiprows=1, na_values=-999.0)\n",
    "    tmp = pd.read_csv(filepath, header=0)\n",
    "    tmp.columns = ['time','TWC_robust']\n",
    "    tmp.loc[:,'time'] = pd.Timestamp(tmpdate) + pd.to_timedelta(tmp['time'],errors='coerce') # Get rid of null lines\n",
    "    tmp.set_index(keys='time',inplace=True,verify_integrity=False)\n",
    "    tmp = tmp[pd.notnull(tmp.index)]\n",
    "    tmp.index.rename('timeutc',inplace=True)\n",
    "\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in Robust is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    return tmp\n",
    "\n",
    "robustset = [ read_robust(filepath) for filepath \n",
    "          in glob.glob('/data/mcfarq/a/szhu28/research/HIWC/data/fulldataCayenne/20170513lamp/robust/*V2*') ]\n",
    "# astype does not provide a 'coerce' option\n",
    "datasetrobust = pd.DataFrame().append(robustset).apply(pd.to_numeric, axis=0, errors='coerce').dropna()\n",
    "datasetrobust.to_xarray().to_netcdf(outputfile,mode='a',\n",
    "    format='NETCDF4',engine='netcdf4',group='/robust',unlimited_dims=['timeutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T14:10:51.625009Z",
     "start_time": "2017-05-15T14:10:44.882183Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CDP Conc & PSD\n",
    "def read_cdpconc(filepath):\n",
    "    tmp = os.path.basename(filepath)\n",
    "    tmpflight = int(tmp[24:26])\n",
    "    tmpdate = meta[tmpflight]\n",
    "    tmp = pd.read_csv(filepath)\n",
    "\n",
    "    tmp.loc[:,'time'] = (pd.TimedeltaIndex(tmp.time,unit='s') + pd.Timestamp(tmpdate)).round('s')\n",
    "    tmp.set_index(keys='time',inplace=True,verify_integrity=False)\n",
    "    tmp = tmp[pd.notnull(tmp.index)]\n",
    "    tmp.index.rename('timeutc',inplace=True)\n",
    "\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in saffire is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    return tmp\n",
    "\n",
    "tmpset = [ read_cdpconc(filepath) for filepath \n",
    "          in glob.glob('/data/mcfarq/a/szhu28/research/HIWC/data/fulldataCayenne/20170513lamp/cdp/Conc*') ]\n",
    "# dropna applies changes\n",
    "datasetcdpconc = pd.DataFrame().append(tmpset).apply(pd.to_numeric,errors='raise').dropna(how='all')\n",
    "# datasetcdpconc.to_xarray().to_netcdf('tmp/cayenne_bulk.nc',mode='a',\n",
    "#     format='NETCDF4',engine='netcdf4',group='/cdp',unlimited_dims=['timeutc'])\n",
    "\n",
    "## CDP PSD\n",
    "def read_cdppsd(filepath):\n",
    "    tmp = os.path.basename(filepath)\n",
    "    tmpflight = int(tmp[23:25])\n",
    "    tmpdate = meta[tmpflight]\n",
    "    tmp = pd.read_csv(filepath)\n",
    "\n",
    "    tmp.loc[:,'time'] = (pd.TimedeltaIndex(tmp.time,unit='s') + pd.Timestamp(tmpdate)).round('s')\n",
    "    tmp.set_index(keys='time',inplace=True,verify_integrity=False)\n",
    "    tmp = tmp[pd.notnull(tmp.index)]\n",
    "    tmp.index.rename('timeutc',inplace=True)\n",
    "\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in saffire is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    return tmp\n",
    "\n",
    "tmpset = [ read_cdppsd(filepath) for filepath \n",
    "          in glob.glob('/data/mcfarq/a/szhu28/research/HIWC/data/fulldataCayenne/20170513lamp/cdp/PSD*') ]\n",
    "# dropna applies changes\n",
    "datasetcdppsd = pd.DataFrame().append(tmpset).apply(pd.to_numeric,errors='raise').dropna(how='all')\n",
    "\n",
    "# Combine and create xr.Dataset\n",
    "b = xr.DataArray(datasetcdppsd)\n",
    "b = b.rename({'dim_1':'bin_mid_cdp'})\n",
    "b.bin_mid_cdp.values = b.bin_mid_cdp.astype(float)\n",
    "b.bin_mid_cdp.attrs['unit']='um'\n",
    "c= datasetcdpconc.to_xarray()\n",
    "c['psd']=b\n",
    "c.to_netcdf(outputfile,mode='a',\n",
    "    format='NETCDF4',engine='netcdf4',group='/cdp',unlimited_dims=['timeutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## LAMP PSD\n",
    "def read_lamppsd(filepath,tmpflight,key):\n",
    "    tmpdate = meta[tmpflight]\n",
    "    tmp = pd.read_csv(filepath,sep=r\"\\s+\",header=None,skiprows=1)\n",
    "\n",
    "    tmp.iloc[:,0] = pd.TimedeltaIndex(tmp.iloc[:,0],unit='s') + pd.Timestamp(tmpdate)\n",
    "    tmp.set_index(keys=0,inplace=True,verify_integrity=False)\n",
    "    tmp = tmp[pd.notnull(tmp.index)]\n",
    "    tmp.index.rename('timeutc',inplace=True)\n",
    "\n",
    "    if tmp.index.is_unique is False:\n",
    "        print('Warning!! Duplicate data in lamppsd is found, dropping ->')\n",
    "        print(sum(tmp.index.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        tmp = tmp[~tmp.index.duplicated(keep='last')]\n",
    "    \n",
    "    tmp = pd.concat([tmp], axis=1, keys=[key]) # Add the outmost row index flight number\n",
    "\n",
    "    return tmp\n",
    "\n",
    "# Generate fileinfo for lamppsd\n",
    "lamproot = '/data/mcfarq/a/szhu28/research/HIWC/data/fulldataCayenne/20170513lamp/dataHAIC/'\n",
    "paras = np.array([\n",
    "    ['msddeq','msddmax','psddeq','psddmax','psdly'],\n",
    "    ['*MassSize*Deq*','*MassSize*Dmax*','*Composite*Deq*','*Composite*Dmax*','*Composite*Ly*']])\n",
    "\n",
    "tmpinfo = []\n",
    "for i in range(paras.shape[1]):\n",
    "    wildcard = paras[1][i]\n",
    "    filepaths = glob.glob(lamproot+wildcard)\n",
    "    filepaths.sort()\n",
    "    tmpset = []\n",
    "    for filepath in filepaths:\n",
    "        tmp = os.path.basename(filepath)\n",
    "        m = re.search('(?<=_VOL)\\d\\d(?=-)', tmp)\n",
    "        tmpflight = int(m.group(0))\n",
    "        tmpinfo.append({'filepath':filepath,'tmpflight':tmpflight,'key':paras[0][i]})\n",
    "fileinfo = np.array(tmpinfo).reshape(paras.shape[1],-1)\n",
    "\n",
    "# In the sense of psd binned\n",
    "tmpdataset = []\n",
    "for i in range(fileinfo.shape[0]):\n",
    "    tmpset = []\n",
    "    for j in range(fileinfo.shape[1]):\n",
    "        tmpset.append(read_lamppsd(**fileinfo[i][j]))\n",
    "    tmp = pd.DataFrame().append(tmpset)\n",
    "    tmpdataset.append( tmp )\n",
    "\n",
    "b = pd.concat(tmpdataset,axis=1)\n",
    "b = pd.Panel({x:b[x] for x in b.columns.levels[0]}).to_xarray()\n",
    "\n",
    "b = b.rename({'major_axis':'timeutc','minor_axis':'bin_mid_composite'})\n",
    "b = b.to_dataset(dim='items')\n",
    "b.to_netcdf(outputfile,mode='a',\n",
    "    format='NETCDF4',engine='netcdf4',group='/lamp',unlimited_dims=['timeutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T20:58:30.832356Z",
     "start_time": "2017-05-15T20:56:50.021726Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge all data to one time dimension\n",
    "setnamelist = ['saffire','ikp','robust','cdp','lamp']\n",
    "# setnamelist = ['saffire','ikp','robust','cdp']\n",
    "alldata = []\n",
    "for name in setnamelist:\n",
    "    alldata.append( xr.open_dataset('tmp/cayenne_bulk.nc',group=name) )\n",
    "\n",
    "alltimelist = [x.timeutc for x in alldata]\n",
    "alltime = unique(xr.concat(alltimelist,dim='timeutc'))\n",
    "alltime = xr.DataArray(alltime,dims={'timeutc':len(alltime)},coords={'timeutc':alltime})\n",
    "alltime = alltime.to_dataset(name='time_to_drop')\n",
    "\n",
    "outputfile = 'tmp/cayenne_sync_bulk.nc'\n",
    "for i in range(len(setnamelist)-1):\n",
    "    name = setnamelist[i]\n",
    "#     mode = 'a'\n",
    "    if i == 0:\n",
    "        mode = 'w'\n",
    "    else:\n",
    "        mode = 'a'\n",
    "    xr.merge([alltime,alldata[i]]).drop('time_to_drop').to_netcdf(outputfile,mode=mode,\n",
    "    format='NETCDF4',engine='netcdf4',group='/'+name,unlimited_dims=['timeutc'])\n",
    "\n",
    "i = 4\n",
    "name = setnamelist[i]\n",
    "outputfile = 'tmp/cayenne_sync_gz.nc'\n",
    "xr.merge([alltime,alldata[i]]).drop('time_to_drop').to_netcdf(outputfile,mode='w',\n",
    "    format='NETCDF4',engine='netcdf4',group='/'+name,unlimited_dims=['timeutc'],\n",
    "    encoding = {x:{'zlib':True} for x in ['msddeq','msddmax','psddeq','psddmax','psdly']} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-15T21:32:56.642269Z",
     "start_time": "2017-05-15T21:32:56.620327Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Elaborate coordinates and attributes\n",
    "totalbins = 1284\n",
    "bin_div = 10*np.arange(totalbins+1)+10\n",
    "bin_mid = (bin_div[:-1]+bin_div[1:])/2\n",
    "with netCDF4.Dataset('tmp/cayenne_sync_gz.nc','a') as dset:\n",
    "    dset['/lamp/bin_mid_composite'][:] = bin_mid\n",
    "    dset['/lamp'].bin_div = bin_div.astype(float)\n",
    "    dset['/lamp'].bin_div_units = 'um'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-17T14:06:51.358837Z",
     "start_time": "2017-05-17T14:06:51.221804Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### IGF routine\n",
    "f_mygamma = lambda nml, x: 10**nml[0]*x**nml[1]*np.exp(-nml[2]*x)\n",
    "def f_one_mode(psd, bin_div, moments):\n",
    "    # bin_diff = np.diff(bin_div)\n",
    "    # bin_mid = (bin_div[:-1]+bin_div[1:])/2.\n",
    "    mobs = np.empty(moments.shape)\n",
    "    for szmoment in range(len(mobs)):\n",
    "        mobs[szmoment] = np.sum( psd*bin_diff*bin_mid**(moments[szmoment]) )\n",
    "\n",
    "    x0 = np.array([log10(300), -1, 0.0014])\n",
    "    tmpind = psd>0\n",
    "    tmpfun = lambda x, nml1, nml2, nml3: f_mygamma(np.array([nml1,nml2,nml3]),x)\n",
    "    nml0, pcov = scipy.optimize.curve_fit(tmpfun, bin_mid[tmpind], psd[tmpind], p0=x0)\n",
    "    # nml0 = [1.8744, -0.6307, 0.0033]\n",
    "    f_fit = lambda nml: f_sum_chisquare(nml, moments, bin_mid, bin_diff, mobs)\n",
    "    optresult = scipy.optimize.minimize(f_fit, nml0, method='Nelder-Mead')\n",
    "    return optresult\n",
    "    \n",
    "def f_sum_chisquare( nml, moments, bin_mid, bin_diff, mobs ):\n",
    "    psd = f_mygamma(nml, bin_mid)\n",
    "    ### We may drop this condition if the code works fine.\n",
    "#     if any(np.isnan(psd)) or any(np.isinf(psd)):\n",
    "#         return np.inf\n",
    "    mfit = np.empty(mobs.shape)\n",
    "    for szmoment in range(len(mfit)):\n",
    "        mfit[szmoment] = np.sum( psd*bin_diff*bin_mid**(moments[szmoment]) )\n",
    "    return np.sum( (mfit-mobs)**2/mobs/mfit )\n",
    "\n",
    "## For median mass diameter calculation\n",
    "def f_mmd(bin_div, msd):\n",
    "    # bin_diff = np.diff(bin_div)\n",
    "    cmsd = np.concatenate( (np.array([0]),np.cumsum(msd*bin_diff)) )\n",
    "    if cmsd[-1]<=0:\n",
    "        return np.NaN\n",
    "    cmsd /= cmsd[-1]\n",
    "    indtmp = np.where(np.diff(cmsd>0.5)==1)[0]\n",
    "    x1,x2,y1,y2 = bin_div[indtmp], bin_div[indtmp+1], cmsd[indtmp], cmsd[indtmp+1]\n",
    "    mmd = (x2-x1)/(y2-y1)*(0.5-y1)+x1\n",
    "    return mmd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-17T14:02:39.860672Z",
     "start_time": "2017-05-17T14:02:11.975034Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IGF fit\n",
    "a = xr.open_dataset('tmp/cayenne_sync_gz.nc',group='/lamp')\n",
    "bin_div = a.bin_div\n",
    "bin_diff = np.diff(bin_div)\n",
    "bin_mid = (bin_div[:-1]+bin_div[1:])/2.\n",
    "\n",
    "## IGF fit and save the data\n",
    "moments = np.array([0,2,3])\n",
    "PSDs = a['psddmax'].values\n",
    "MSDs = a['msddmax'].values\n",
    "shp = PSDs.shape\n",
    "# At least 11 non-zero bins are required for a valid PSD\n",
    "validpsdbool = (~any(isnan(PSDs),axis=1)) & (sum(PSDs>0,axis=1)>10)\n",
    "validpsdind = np.where(validpsdbool)[0]\n",
    "\n",
    "'''\n",
    "import signal\n",
    "class TimeoutException(Exception):   # Custom exception class\n",
    "    pass\n",
    "def timeout_handler(signum, frame):   # Custom signal handler\n",
    "    raise TimeoutException\n",
    "# Change the behavior of SIGALRM\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "for i in range(3):\n",
    "    # Start the timer. Once 5 seconds are over, a SIGALRM signal is sent.\n",
    "    signal.alarm(5)    \n",
    "    # This try/except loop ensures that \n",
    "    #   you'll catch TimeoutException when it's sent.\n",
    "    try:\n",
    "        A(i) # Whatever your function that might hang\n",
    "    except TimeoutException:\n",
    "        continue # continue the for loop if function A takes more than 5 second\n",
    "    else:\n",
    "        # Reset the alarm\n",
    "        signal.alarm(0)\n",
    "'''\n",
    "# IGF fit and save data\n",
    "\n",
    "output = {}\n",
    "i = 0\n",
    "totali = len(validpsdind)\n",
    "for szi in validpsdind:\n",
    "    i+=1\n",
    "    psd = PSDs[szi,:]\n",
    "    print(str(szi)+' ... '+str(i/totali),end=\"\")\n",
    "    try:\n",
    "        output[szi]=f_one_mode(psd,bin_div,moments)\n",
    "        print('\\r'+str(szi)+' Done. Now',end=\"\")\n",
    "    except:\n",
    "        output[szi]=None\n",
    "        print('\\r'+str(szi)+' Error. Now',end=\"\")\n",
    "\n",
    "with open('tmp/output.p', 'wb') as file:\n",
    "    pickle.dump(output,file)\n",
    "    print('Output file saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-17T05:03:23.637345Z",
     "start_time": "2017-05-17T05:03:23.242860Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the output fit file and save to cayenne_sync_bulk.nc as /lampproc group\n",
    "with open('tmp/output.p', 'rb') as file:\n",
    "    output = pickle.load(file)\n",
    "    print('Output file read')\n",
    "\n",
    "with netCDF4.Dataset(\"tmp/cayenne_sync_gz.nc\", \"r\", format=\"NETCDF4\") as file:\n",
    "    varname = list(file.groups['lamp'].variables.keys())\n",
    "    varname.remove('timeutc')\n",
    "frame = xr.open_dataset('tmp/cayenne_sync_gz.nc',group='/lamp',drop_variables=varname)\n",
    "\n",
    "tmpnml = np.empty((frame.dims['timeutc'],3), dtype=float)\n",
    "tmpnml[:] = nan\n",
    "output = {k:v.x for k,v in output.items() if v is not None}\n",
    "tmpnml[ list(output.keys()),:] = np.array([x for x in output.values()])\n",
    "tmpvalidbinnum = np.empty(PSDs.shape[0])\n",
    "tmpvalidbinnum[:] = nan\n",
    "mask = ~any(isnan(PSDs),axis=1)\n",
    "tmpvalidbinnum[mask] = np.sum(PSDs[mask,:]>0,axis=1)\n",
    "frame['validbinnum'] = xr.DataArray(tmpvalidbinnum,dims=['timeutc'])\n",
    "frame['nml'] = xr.DataArray(tmpnml,dims=['timeutc','dimnml'])\n",
    "\n",
    "frame.to_netcdf('tmp/cayenne_sync_bulk.nc',mode='a',\n",
    "    format='NETCDF4',engine='netcdf4',group='/lampproc',unlimited_dims=['timeutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-17T14:22:32.987662Z",
     "start_time": "2017-05-17T14:22:30.669687Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add mmd to cayenne_sync_bulk.nc, also as an template for adding variable\n",
    "tmpmmd = np.empty(PSDs.shape[0])\n",
    "tmpmmd[:] = nan\n",
    "validMSDs = MSDs[validpsdind,:]\n",
    "validmmd = np.array([f_mmd(bin_div,x) for x in validMSDs])\n",
    "tmpmmd[validpsdind] = validmmd\n",
    "\n",
    "# As a template to add new variable to existing netcdf file instead of using xarray\n",
    "file = netCDF4.Dataset('tmp/cayenne_sync_bulk.nc',mode='a')\n",
    "grp = file['/lampproc']\n",
    "# The extra comma makes sure the passed constant is a tuple as required\n",
    "# Once variable created, the file will reject repeated creation\n",
    "# varmmd = grp.createVariable(\"mmd\",\"f8\",('timeutc',))\n",
    "varmmd = grp['mmd']\n",
    "varmmd[:] = tmpmmd\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-02T06:13:29.662110Z",
     "start_time": "2017-06-02T05:58:33.042016Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing key latitude ...\n",
      "Processing key longitude ...\n",
      "Processing key altitude ...\n",
      "Processing key pitch ...\n",
      "Processing key roll ...\n",
      "Processing key drift ...\n",
      "Processing key heading ...\n",
      "Processing key track ...\n",
      "Processing key aircraft_vh ...\n",
      "Processing key aircraft_vz ...\n",
      "Processing key pressure ...\n",
      "Processing key temperature ...\n",
      "Processing key relative_humidity ...\n",
      "Processing key eastward_wind ...\n",
      "Processing key northward_wind ...\n",
      "Processing key u_wind ...\n",
      "Processing key v_wind ...\n",
      "Processing key w_wind ...\n",
      "Processing key u_wind_fuselage ...\n",
      "Processing key v_wind_fuselage ...\n",
      "Processing key land_water_flag ...\n",
      "Processing key height_2D ...\n",
      "Processing key Z_vertical ...\n",
      "Processing key Z_L1_vertical ...\n",
      "Processing key V_vertical ...\n",
      "Processing key R_vertical ...\n",
      "Processing key latitude_vertical ...\n",
      "Processing key longitude_vertical ...\n",
      "Processing key azimuth_east_vertical ...\n",
      "Processing key elevation_hor_vertical ...\n",
      "Processing key Z_backward ...\n",
      "Processing key Z_L1_backward ...\n",
      "Processing key V_backward ...\n",
      "Processing key R_backward ...\n",
      "Processing key latitude_backward ...\n",
      "Processing key longitude_backward ...\n",
      "Processing key azimuth_east_backward ...\n",
      "Processing key elevation_hor_backward ...\n",
      "Processing key Z_transverse ...\n",
      "Processing key Z_L1_transverse ...\n",
      "Processing key V_transverse ...\n",
      "Processing key R_transverse ...\n",
      "Processing key latitude_transverse ...\n",
      "Processing key longitude_transverse ...\n",
      "Processing key azimuth_east_transverse ...\n",
      "Processing key elevation_hor_transverse ...\n",
      "Processing key distance_vertical_backward ...\n",
      "Processing key distance_vertical_tranverse ...\n",
      "Processing key Z ...\n",
      "Processing key Vx ...\n",
      "Processing key Vy ...\n",
      "Processing key Vz ...\n",
      "Processing key VE ...\n",
      "Processing key VN ...\n",
      "Processing key Mask_Vx ...\n",
      "Processing key Mask_Vy ...\n",
      "Processing key Mask_Vz ...\n",
      "Processing key Vx_error ...\n",
      "Processing key Vy_error ...\n",
      "Processing key Vz_error ...\n",
      "Processing key Temperature_field ...\n",
      "Processing key Pressure_field ...\n",
      "Processing key Mask_domain ...\n",
      "Processing key convective_index ...\n",
      "Processing key Mask_wind ...\n",
      "Processing key attenuation_phase_flag ...\n",
      "Processing key RainRate ...\n",
      "Processing key Gaseous_twowayatt ...\n",
      "Processing key w_ret ...\n",
      "Processing key iwc_ret ...\n",
      "Processing key Vt_ret ...\n",
      "Processing key Dm_ret ...\n",
      "Processing key N0_ret ...\n",
      "Processing key iwc_IWC_Z_T ...\n",
      "Processing key extinction_ret ...\n",
      "Processing key re_ret ...\n",
      "Processing key Nt_ret ...\n",
      "Processing key Z_in ...\n",
      "Processing key V_in ...\n",
      "Processing key T_in ...\n",
      "Processing key Z_fwd ...\n",
      "Processing key Z_noatt_fwd ...\n",
      "Processing key V_fwd ...\n",
      "Processing key Z_Xband ...\n",
      "Processing key error_v ...\n",
      "Processing key error_lnz ...\n",
      "Processing key lniwc_apriori ...\n",
      "Processing key error_lniwc_apriori ...\n",
      "Processing key Jd ...\n",
      "Processing key Ju ...\n",
      "Processing key iJd ...\n",
      "Processing key iJu ...\n",
      "Processing key lniwc_error ...\n",
      "Processing key w_error ...\n"
     ]
    }
   ],
   "source": [
    "# Destructively concatenate RASTA data to the merged dataset\n",
    "outputfile = 'tmp/cayenne_rasta.nc'\n",
    "# Read RASTA files\n",
    "def read_rasta(filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    tmpflight = int(re.search('(?<=_F)\\d*(?=_radonvar)',filename).group())\n",
    "    rasta = xr.open_dataset(filepath)\n",
    "\n",
    "    sec = 3600*rasta.time.values\n",
    "    time = (pd.TimedeltaIndex(sec,unit='s') + pd.Timestamp(meta[tmpflight])).round('s')\n",
    "    rasta['time'] = time\n",
    "\n",
    "    if time.is_unique is False:\n",
    "        print('Warning!! Duplicate data in saffire is found, dropping ->')\n",
    "        print(sum(time.duplicated()))\n",
    "        ### Dropping duplicate based on index, see\n",
    "        ### http://pandas.pydata.org/pandas-docs/stable/indexing.html#duplicate-data\n",
    "        uniindex = np.where(~time.duplicated(keep='last'))[0]\n",
    "        rasta = rasta.isel(time=uniindex)\n",
    "    \n",
    "    rasta.rename({'time':'timeutc'},inplace=True)\n",
    "    return rasta\n",
    "\n",
    "rastafilenames = glob.glob('/data/mcfarq/a/szhu28/research/HIWC/data/fulldataCayenne/20170513lamp/rasta/*.nc')\n",
    "\n",
    "rastaset = [ read_rasta(filepath) for filepath in rastafilenames ]\n",
    "timeutc = xr.open_dataset('tmp/cayenne/cayenne_sync_bulk.nc',group='/saffire').timeutc\n",
    "\n",
    "key = 'timeutc'\n",
    "xr.Dataset(data_vars={'timeutc':timeutc}).to_netcdf(outputfile,mode='w', ## 'w' for first time, later change to 'a'\n",
    "    format='NETCDF4',engine='netcdf4',group='/',unlimited_dims=['timeutc'])\n",
    "\n",
    "keys = list(rastaset[0].data_vars.keys())\n",
    "tot = len(keys)\n",
    "i = 0\n",
    "for key in keys:\n",
    "    i+=1\n",
    "    print('Processing key '+key+' ... '+str(i)+'/'+str(tot))\n",
    "    tmpset = [ x[key] for x in rastaset ]\n",
    "\n",
    "    datasettmp = xr.concat(tmpset,dim='timeutc')\n",
    "    datasettmp = xr.align(datasettmp,indexes={'timeutc':timeutc})[0]\n",
    "    datasettmp.to_netcdf(outputfile,mode='a', ## 'w' for first time, later change to 'a'\n",
    "        format='NETCDF4',engine='netcdf4',group='/'+key,unlimited_dims=['timeutc'],\n",
    "        encoding={ key:{'zlib':True, 'complevel':1} } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-05T22:09:31.007795Z",
     "start_time": "2017-06-05T22:09:28.816554Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct link table to MTSAT file\n",
    "dirpaths = glob.glob('/data/gpm/a/shared/szhu28/hiwcproc/cayenne/mtsat/*')\n",
    "dirpaths.sort()\n",
    "\n",
    "allfiles = []\n",
    "for dirpath in dirpaths:\n",
    "    datestr = os.path.basename(dirpath)\n",
    "    filepaths = glob.glob(dirpath+'/*')\n",
    "    filepaths.sort()\n",
    "    for filepath in filepaths:\n",
    "        filename = os.path.basename(filepath)\n",
    "        timestr = os.path.splitext(filename)[0]\n",
    "        allfiles.append((datestr+timestr,filepath))\n",
    "\n",
    "timesat = pd.DatetimeIndex( [ x[0] for x in allfiles ] )\n",
    "satfiles = np.array( [ x[1] for x in allfiles ] )\n",
    "satfiles = xr.DataArray(satfiles, dims=['timesat'], coords={'timesat':timesat}, name='satfiles')\n",
    "\n",
    "sf = xr.open_dataset('tmp/cayenne/cayenne_sync_bulk.nc',group='/saffire')[ ['latitude','longitude'] ]\n",
    "timeutc = sf.timeutc.values\n",
    "orig = satfiles.timesat.values\n",
    "insind = np.searchsorted(orig,timeutc)\n",
    "torf = abs(timeutc-orig[insind-1])>abs(timeutc-orig[insind])\n",
    "inserted = np.where(torf, orig[insind], orig[insind-1])\n",
    "\n",
    "indarr = np.where(np.insert(diff(inserted),0,0))[0]\n",
    "indarr = np.concatenate([[0],indarr,[None]])\n",
    "\n",
    "varkeys = [\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'reflectance_vis',\n",
    " 'visible_count',\n",
    " 'reflectance_nir',\n",
    " 'temperature_sir',\n",
    " 'temperature_67',\n",
    " 'temperature_ir',\n",
    " 'temperature_sw',\n",
    " 'broadband_shortwave_albedo',\n",
    " 'broadband_longwave_flux',\n",
    " 'cloud_ir_emittance',\n",
    " 'cloud_phase',\n",
    " 'cloud_visible_optical_depth',\n",
    " 'cloud_particle_size',\n",
    " 'cloud_lwp_iwp',\n",
    " 'cloud_effective_temperature',\n",
    " 'cloud_top_pressure',\n",
    " 'cloud_effective_pressure',\n",
    " 'cloud_bottom_pressure',\n",
    " 'cloud_top_height',\n",
    " 'cloud_effective_height',\n",
    " 'cloud_bottom_height',\n",
    " 'cloud_top_temperature',\n",
    " 'cloud_bottom_temperature',\n",
    " 'pixel_skin_temperature',\n",
    "]\n",
    "\n",
    "satds = xr.Dataset(data_vars={'timeutc':timeutc})\n",
    "\n",
    "def f_dist(x1,y1,x2,y2):\n",
    "    # remember to switch to radial before using\n",
    "    x1,y1,x2,y2 = [ f_rad(x) for x in [x1,y1,x2,y2] ]\n",
    "    return np.sqrt(((x1-x2)*np.cos((y1+y2)/2))**2 + (y1-y2)**2)\n",
    "def f_rad(x):\n",
    "    return x/180*np.pi\n",
    "\n",
    "def f_4ptinterp(xi,yi,xs,ys,zs):\n",
    "    dists = f_dist(xi,yi,xs,ys)\n",
    "    return np.average(zs,weights=np.minimum(1/dists,1e10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-05T23:33:43.693940Z",
     "start_time": "2017-06-05T23:21:59.405556Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "# Process each MTSAT files and generate along-flight dataset\n",
    "toconcat = []\n",
    "outputfile = 'tmp/cayenne_mtsat.nc'\n",
    "warnings.filterwarnings('ignore')\n",
    "for i in range(len(indarr)-1):\n",
    "    print(i)\n",
    "# for i in range(2):\n",
    "    indi = slice(indarr[i],indarr[i+1])\n",
    "    filepath = str(satfiles.sel(timesat=inserted[indarr[i]]).values)\n",
    "\n",
    "    # Gladly in Cayenne dataset missing_value attribute works fine compared to Darwin\n",
    "    tmpds = xr.open_dataset(filepath, mask_and_scale=True)\n",
    "    tmpds = tmpds[varkeys]\n",
    "\n",
    "    frameds = sf.isel(timeutc=indi)\n",
    "\n",
    "    lons = tmpds['longitude'].values\n",
    "    lats = tmpds['latitude'].values\n",
    "    lon = frameds.longitude.values\n",
    "    lat = frameds.latitude.values\n",
    "\n",
    "    # Generate two arrays for lon, lat\n",
    "    # 1. smallest value greater than all the elements before (inclusive) the current line (sgtb)\n",
    "    # 2. greatest value smaller than all the elements after (inclusive) the current line (gsta)\n",
    "\n",
    "    # For latitude, the generate trend is DECREASING wrt. index increasing.\n",
    "    tmp = np.nanmax(lons,axis=0)\n",
    "    tmp[isnan(tmp)] = -inf\n",
    "    tmp = np.maximum.accumulate(tmp)\n",
    "    tmpind = np.nonzero(tmp==-inf)[0][-1]\n",
    "    tmp[:tmpind+1] = tmp[tmpind+1]\n",
    "    sgtblon = tmp\n",
    "\n",
    "    tmp = np.nanmin(lons,axis=0)\n",
    "    tmp[isnan(tmp)] = inf\n",
    "    tmp = np.minimum.accumulate(tmp[::-1])[::-1]\n",
    "    tmpind = np.nonzero(tmp==inf)[0][0]\n",
    "    tmp[tmpind:] = tmp[tmpind-1]\n",
    "    gstalon = tmp\n",
    "\n",
    "    # any(gstalon>sgtblon) This should be false if the above codes work fine.\n",
    "\n",
    "    # For latitude, the generate trend is DECREASING wrt. index increasing.\n",
    "    tmp = np.nanmax(lats,axis=1)\n",
    "    tmp[isnan(tmp)] = -inf\n",
    "    tmp = np.maximum.accumulate(tmp[::-1])[::-1]\n",
    "    tmpind = np.nonzero(tmp==-inf)[0][0]\n",
    "    tmp[tmpind:] = tmp[tmpind-1]\n",
    "    sgtblat = tmp\n",
    "\n",
    "    tmp = np.nanmin(lats,axis=1)\n",
    "    tmp[isnan(tmp)] = inf\n",
    "    tmp = np.minimum.accumulate(tmp)\n",
    "    tmpind = np.nonzero(tmp==inf)[0][-1]\n",
    "    tmp[:tmpind+1] = tmp[tmpind+1]\n",
    "    gstalat = tmp\n",
    "    # any(gstalat>sgtblat) This should be false if the above codes work fine.\n",
    "\n",
    "    # gsta -> upper bound, sgtb -> lower bound\n",
    "    lonr = np.searchsorted(gstalon, lon)+1 # +1 is for the upper bound exclusive in python\n",
    "    lonl = np.searchsorted(sgtblon, lon)-1 # -1 is for considering all possibility\n",
    "\n",
    "    # gsta -> upper bound, sgtb -> lower bound\n",
    "    latl = len(gstalat)-np.searchsorted(gstalat[::-1], lat)-1 # -1 is for considering all possibility\n",
    "    latr = len(sgtblat)-np.searchsorted(sgtblat[::-1], lat)+1 # +1 is for the upper bound exclusive in python\n",
    "\n",
    "    # Add another wrapper for key values loop\n",
    "    toaddds = {}\n",
    "    for key in varkeys:\n",
    "        tmpzs = tmpds[key].values\n",
    "        zint = []\n",
    "        for j in range(len(lon)):\n",
    "            indsubgrid = slice(latl[j],latr[j]),slice(lonl[j],lonr[j])\n",
    "            x = lons[indsubgrid]\n",
    "            if len(x) == 0:\n",
    "                zint.append(nan)\n",
    "                continue\n",
    "            y = lats[indsubgrid]\n",
    "            z = tmpzs[indsubgrid]\n",
    "            indmin = np.unravel_index( ((x-lon[j])**2+(y-lat[j])**2).argmin(), x.shape)\n",
    "\n",
    "            indnine = slice(indmin[0]-1,indmin[0]+2),slice(indmin[1]-1,indmin[1]+2)\n",
    "            xs = x[indnine].ravel()\n",
    "            if len(xs) == 0:\n",
    "                zint.append(nan)\n",
    "                continue\n",
    "            ys = y[indnine].ravel()\n",
    "            zs = z[indnine].ravel()\n",
    "            zint.append(f_4ptinterp(lon[j],lat[j],xs,ys,zs))\n",
    "        zint = np.array(zint)\n",
    "        toaddds[key] = xr.DataArray(zint, dims=['timeutc'], coords={'timeutc':frameds.timeutc},\n",
    "                                    attrs=tmpds[key].attrs, name=key)\n",
    "    toconcat.append(xr.Dataset(toaddds))\n",
    "\n",
    "warnings.filterwarnings('default') # restore default settings\n",
    "\n",
    "mtsatproc = xr.concat(toconcat,dim='timeutc')\n",
    "\n",
    "# Remember to add the variable for time difference and a mask\n",
    "tmpmtsatproc = xr.Dataset({},coords={'timeutc':timeutc})\n",
    "tmp = timeutc - inserted\n",
    "tmpmtsatproc['timelag'] = xr.DataArray(tmp, dims=['timeutc'], coords={'timeutc':timeutc})\n",
    "# Note the threshold is 30 min for Darwin and 15 min for Cayenne\n",
    "tmpmtsatproc['validlagmask'] = xr.DataArray( tmp <= np.timedelta64(15,'m') , dims=['timeutc'], coords={'timeutc':timeutc})\n",
    "mtsatproc = xr.merge([tmpmtsatproc,mtsatproc],join='left')\n",
    "mtsatproc.to_netcdf(outputfile,mode='w',\n",
    "    format='NETCDF4',engine='netcdf4',group='/mtsatproc',unlimited_dims=['timeutc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert and create NCAR ECMWF archive\n",
    "import pygrib\n",
    "import resource\n",
    "\n",
    "# Parameters for Cayenne\n",
    "filepaths = glob.glob('/data/gpm/a/shared/szhu28/hiwcproc/era_interim/2015**/*regn128sc*',recursive=True)\n",
    "kw = {'lat1':0,'lat2':10,'lon1':-60+360,'lon2':-45+360}\n",
    "outputnoext = 'tmp/cayenne/ecmwf'\n",
    "\n",
    "filename = '/net/san-b8-ib/data/gpm/a/shared/szhu28/hiwcproc/era_interim/201402/ei.oper.an.pl.regn128sc.2014020306'\n",
    "# with pygrib.open(filename) as grbs:\n",
    "grbs = pygrib.open(filename)\n",
    "grbs.rewind()\n",
    "varfullnames = {x.shortName:(x.name, x.units) for x in grbs}\n",
    "numofvar = len(varfullnames)\n",
    "grbs.rewind()\n",
    "varnames = [x.shortName for x in grbs[0:numofvar]]\n",
    "\n",
    "# [ (i,varnames[i],varfullnames[varnames[i]]) for i in range(numofvar) ]\n",
    "\n",
    "filename = '/net/san-b8-ib/data/gpm/a/shared/szhu28/hiwcproc/era_interim/201402/ei.oper.an.pl.regn128uv.2014020306'\n",
    "# with pygrib.open(filename) as grbs:\n",
    "grbsuv = pygrib.open(filename)\n",
    "grbsuv.rewind()\n",
    "varfullnamesuv = {x.shortName:(x.name, x.units) for x in grbsuv}\n",
    "numofvaruv = len(varfullnamesuv)\n",
    "grbsuv.rewind()\n",
    "varnamesuv = [x.shortName for x in grbsuv[0:numofvaruv]]\n",
    "\n",
    "# [ (i,varnamesuv[i],varfullnamesuv[varnamesuv[i]]) for i in range(numofvaruv) ]\n",
    "\n",
    "grbs.rewind()\n",
    "level = np.array([x.level for x in grbs[0::numofvar] ]).astype(float)\n",
    "\n",
    "key = 'level'\n",
    "dimlvl = xr.DataArray(level, dims=['level'], coords={'level':level}, attrs={\n",
    "    'units':'hPa'}, name=key)\n",
    "\n",
    "_dump, lats, lons = grbs[1].data(**kw)\n",
    "lat, lon = lats[:,0], lons[0,:]\n",
    "key = 'lat'\n",
    "dimlat = xr.DataArray(lat, dims=['lat'], coords={'lat':lat}, attrs={\n",
    "    'units':'degree north'}, name=key)\n",
    "key = 'lon'\n",
    "dimlon = xr.DataArray(lon, dims=['lon'], coords={'lon':lon}, attrs={\n",
    "    'units':'degree east'}, name=key)\n",
    "\n",
    "encoding = dict(zip([varfullnames[x][0] for x in varnames],[{'zlib':True, 'complevel':1}]*len(varnames)))\n",
    "encodinguv = dict(zip([varfullnamesuv[x][0] for x in varnamesuv],[{'zlib':True, 'complevel':1}]*len(varnamesuv)))\n",
    "encoding = {**encoding,**encodinguv}\n",
    "\n",
    "def f_read_model_pair(filepair):\n",
    "    grbs = pygrib.open(filepair[0])\n",
    "    grbsuv = pygrib.open(filepair[1])\n",
    "    timemod = str(grbs[1].dataDate)+\"{:04d}\".format(grbs[1].dataTime)\n",
    "    tmponefile = {}\n",
    "    for skey in varnames:\n",
    "        key = varfullnames[skey][0]\n",
    "        units = varfullnames[skey][1]\n",
    "        tmpdata = np.array([ x.data(**kw)[0] for x in grbs[varnames.index(skey)::numofvar] ])\n",
    "        tmponefile[key] = xr.DataArray(tmpdata, coords=[dimlvl,dimlat,dimlon], attrs={'units':units}, name=key)\n",
    "    for skey in varnamesuv:\n",
    "        key = varfullnamesuv[skey][0]\n",
    "        units = varfullnamesuv[skey][1]\n",
    "        tmpdata = np.array([ x.data(**kw)[0] for x in grbsuv[varnamesuv.index(skey)::numofvaruv] ])\n",
    "        tmponefile[key] = xr.DataArray(tmpdata, coords=[dimlvl,dimlat,dimlon], attrs={'units':units}, name=key)\n",
    "\n",
    "    onefile = xr.Dataset(tmponefile, coords={'level':dimlvl,'lat':dimlat,'lon':dimlon}, attrs={'timemod':timemod})\n",
    "    return onefile, timemod\n",
    "\n",
    "filepaths.sort()\n",
    "filepairs = [ ( x, re.sub('regn128sc','regn128uv',x) ) for x in filepaths ]\n",
    "\n",
    "dslist = []\n",
    "timestrlist = []\n",
    "i=0\n",
    "for x in filepairs:\n",
    "    print(i, resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "    i+=1\n",
    "    ds, timestr = f_read_model_pair(x)\n",
    "    dslist.append(ds)\n",
    "    timestrlist.append(timestr)\n",
    "    \n",
    "with open(outputnoext+'_tmpstore.p','bw') as f:\n",
    "    pickle.dump([dslist,timestrlist],f)\n",
    "    \n",
    "timemods = pd.DatetimeIndex(timestrlist)\n",
    "timemod = xr.DataArray(timemods, dims=['timemod'], coords={'timemod':timemods}, name='timemod')\n",
    "xr.concat(dslist,dim=timemod).to_netcdf(outputnoext+'.nc',mode='w',\n",
    "    format='NETCDF4',engine='netcdf4',group='/', encoding=encoding)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "atmos",
   "language": "python",
   "name": "atmos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
